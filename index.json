[
{
	"uri": "https://phucdat25.github.io/OJT/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Phuc Dat\nPhone Number: 0584825767\nEmail: datnpse182125@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How Heroku Migrated Hundreds of Thousands of Self-Managed PostgreSQL Databases to Amazon Aurora By: Stefan Pieterse, Rocket (John), Jonathan K. Brown, and Justin Downing\nDate: April 10, 2025\nTopics: Advanced (300), Amazon Aurora, Customer Solutions, Migration, PostgreSQL compatible\nIn this post, we discuss how Heroku migrated multi-tenant PostgreSQL databases from a self-managed PostgreSQL environment running on Amazon Elastic Compute Cloud (Amazon EC2) to Amazon Aurora PostgreSQL-Compatible Edition. Heroku completed this large-scale migration without customer impact, while increasing platform reliability and reducing operational burden. We will examine the previous self-managed architecture, the new design, how hundreds of thousands of databases were migrated, and how customer experience improved as a result.\nOverview of Heroku Heroku is a fully managed platform-as-a-service (PaaS) built on Amazon Web Services (AWS), designed to simplify deploying, operating, and scaling applications. Founded in 2007 and acquired by Salesforce in 2010, Heroku is now the platform of choice for more than 13 million applications—from small startups to large-scale enterprise deployments.\nHeroku not only simplifies deployment and scaling using Dynos (Heroku-managed containers) but also offers data service add-ons such as Heroku Postgres, Apache Kafka on Heroku, and Heroku Key-Value Store. These services handle security, server patching, failovers, backups, and other complex configurations, enabling customers to focus on building applications instead of managing data infrastructure. All Heroku Data Services add-ons can be provisioned with a single CLI command or click, providing deep integration, reliability, and scalability.\nOne such add-on is Heroku Postgres, which provides a scalable, cost-effective PostgreSQL database with automated backups, management, performance optimization, and everything required to run a database. For example, Heroku Connect enables seamless synchronization between Salesforce and Heroku Postgres.\nTo ensure customer applications run smoothly, Heroku invests heavily in platform reliability. In early 2025, the Heroku Data Services team migrated its Heroku Postgres Essential tier from a self-managed environment on Amazon EC2 to Amazon Aurora—eliminating operational overhead and allowing engineers to focus on innovation.\nPrevious Self-Managed PostgreSQL Architecture and Its Challenges The diagram below illustrates Heroku’s previous architecture:\nThe Heroku Data team operates multiple control planes to manage customer resources. When a customer provisions a Heroku Postgres add-on, the control plane sends API requests to AWS to create VPCs, EC2 instances, Amazon Elastic Block Store (Amazon EBS) volumes, and Amazon Simple Storage Service (Amazon S3) paths depending on the add-on plan.\nOnce the infrastructure is ready, the control plane configures PostgreSQL services and manages timelines, credentials, and continuous protection. Heroku uses automation to connect to the instances, set configuration files, and execute commands needed to complete database setup. When the database becomes available, the system injects the connection string into the customer’s application configuration. After that, the control plane continuously monitors instance health, service performance, usage, and telemetry to maintain reliability and performance.\nThis architecture served Heroku for more than 10 years. However, managing a massive fleet of database instances introduced growing complexity and operational overhead. Heroku engineers needed to write extensive code to detect and remediate OS issues, handle PostgreSQL upgrades, and address hardware failures.\nEven with automation, engineers increasingly found themselves spending more time on infrastructure maintenance instead of building new customer-facing features. When evaluating alternatives, Amazon Aurora emerged as the clear solution.\nThe New PostgreSQL Architecture with Aurora Here is the illustration of the new design:\nA key advantage is that Heroku’s control plane is already optimized to interact with AWS Cloud services. The system is inspired by a finite-state machine model—each resource exists in a defined state and transitions to another state based on actions. For example, when provisioning an EC2 instance with specific parameters, it enters a pending state until AWS completes the request, then transitions into running.\nBy choosing Aurora as the backend for Heroku Postgres, the Heroku Data team no longer needs to build and manage server infrastructure, maintain custom AMIs, install and patch operating systems, or migrate to new instance types. When customers request a database, Heroku simply provisions and configures an Aurora cluster; AWS handles everything underneath—including instances, storage, replication, and snapshots.\nThis aligns with Heroku’s philosophy of ephemeralization: achieving more with fewer resources. By reducing the complexity of maintaining custom database infrastructure, the team can prioritize delivering higher-value innovations such as enhanced observability, smarter database support, and improved data interoperability.\nHow Heroku Migrated Over 200,000 Databases The Heroku Data team needed to move more than 200,000 self-managed PostgreSQL databases to Aurora—a significant challenge that they completed in just four months with minimal customer disruption.\nAlthough the team had deep PostgreSQL expertise, Aurora was new to them, so AWS collaborated to train more than 40 Heroku engineers.\nThe team used two control planes in parallel:\nLegacy control plane — for the existing self-managed system Modern control plane — for the new Aurora-based system Heroku developed a dedicated transfer system within the modern control plane using pgcopydb to copy data from the old PostgreSQL system to Aurora. Compared with pg_dump/pg_restore, this approach excels thanks to its parallel operations. The average time for lock, copy, pointer update, and unlock was less than 2.2 minutes (p50).\nTo ensure a smooth migration, the team built comprehensive testing capabilities: full end-to-end migration simulations, data validation, and proactive issue handling before customers were impacted. Migrations were executed continuously, averaging 2,000 databases per day. Thanks to meticulous testing, issues were detected and resolved early.\nWhen the transfer system became production-ready, customers had two options:\nSelf-serve migration — customers initiated the migration by changing their plan Automated migration — the system gradually migrated databases in phases to avoid disruption Heroku successfully migrated hundreds of thousands of databases from self-managed PostgreSQL to Aurora PostgreSQL with minimal impact. They also used AWS Countdown, an AWS Enterprise Support program that prepares teams for large-scale events such as migrations and launches, providing quota management, capacity planning, and operational guidance.\nBenefits and Advantages of the New Architecture Moving to the new database architecture represents a long-term investment in an improved customer experience.\nPreviously, operating a fleet of PostgreSQL databases on EC2 required significant engineering effort—effort that competed with building new features. The new Aurora architecture reduces this operational burden: engineers no longer need to manage OS patches, software updates, PostgreSQL services, or supporting libraries.\nOrganizations now rely on AWS to keep customer databases secure, stable, and always available. Heroku gains more time to invest in new features, respond to customer needs, and strengthen its collaboration with AWS.\nThe new architecture enables Heroku Data to deliver future enhancements such as AI-enabled database administration, auto scaling, sleep mode, near-zero downtime, increased connection limits, expanded storage up to 128 TB, and more. With Heroku’s simplicity, attaching such a powerful database to an application requires only a single line of code.\nMigrating to Amazon Aurora also enhances security: encryption at rest via AWS KMS, encryption in transit via SSL/TLS, automatic patching without maintenance windows, IAM authentication, network isolation, and full auditability via AWS CloudTrail. These enterprise-grade security features are built into Aurora by default without additional configuration.\nConclusion The migration from self-managed PostgreSQL on EC2 to Aurora demonstrates Heroku’s careful planning and close collaboration with AWS. After successfully migrating hundreds of thousands of multi-tenant databases, Heroku’s next step is to migrate single-tenant databases within Heroku Private Spaces to Aurora by the end of 2025.\nAbout the Authors Stefan Pieterse is a Principal Customer Solutions Manager at AWS who has supported many strategic customers through cloud migration and modernization. When not working, he enjoys running and spending time with his three-year-old son.\nJohn “Rocket” Nichols is a Solutions Architect at AWS, where he helps large enterprises build systems that are resilient, high-performance, and cost-optimized. He also produces livestream events and makes wine in his spare time.\nJonathan K. Brown is a Senior Product Manager at Heroku, responsible for Heroku Data Services. With a background spanning aerospace to high-tech startups, he holds BS/MS Engineering and an MBA from UC Berkeley, and enjoys paragliding.\nJustin Downing is a Software Engineer Architect at Salesforce, focused on the architecture and systems behind Heroku Data Services. Outside of work, he explores the outdoors through hiking and skiing.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Accelerating Alzheimer’s Research Through Large-Scale Functional Genomics Analysis Powered by AWS Cloud By: Meghan Buder and Karthik Narasimhan\nDate: April 14, 2025\nCategory: Amazon EC2, Customer Solutions, Healthcare, Public Sector\nAlzheimer’s disease is a progressive neurodegenerative disorder affecting millions worldwide. The Alzheimer’s Association estimates that over 6 million Americans are living with the condition, and this number is projected to reach nearly 13 million by 2050. This growing prevalence has driven global research efforts, with scientists working to understand the disease’s complex mechanisms, identify potential risk factors, and develop more effective therapies.\nAt the forefront of this critical research is Dr. Gao Wang, Associate Professor of Neurology at Columbia University. Dr. Wang leads the Lab of Statistical Functional Genomics, where he and his team leverage the power of Amazon Web Services (AWS) cloud computing to conduct groundbreaking genomics research, decoding the complex genetic underpinnings of Alzheimer’s and identifying potential therapeutic targets.\nFunctional Genetics in Alzheimer’s Research Dr. Wang’s research focuses on functional genetics, aiming not only to identify genetic variants associated with Alzheimer’s but also to understand why these variants contribute to disease onset. This approach requires analyzing high-dimensional, large-scale data using sophisticated computational and statistical biology models.\nWith cloud computing, Dr. Wang accelerates analyses, enabling deeper insights and faster progress in understanding Alzheimer’s complex mechanisms. He emphasizes:\n“If the entire field can produce results faster and if those results are meaningful, you can inspire subsequent research and advance the field, hopefully years ahead. Other researchers rely on the data we produce or the findings we generate first, as we can open questions that allow others to focus on their research.”\nThe FunGen-xQTL Project: A Collaborative Effort A major initiative led by Dr. Wang is the FunGen-xQTL Project, a collaboration across 14 research institutions, 28 trainees, and 19 faculty members in the U.S. The project studies molecular quantitative trait loci (QTLs) in aging brains across 62 different molecular contexts in cells and brain tissues.\nThe team generated comprehensive molecular profiles, including DNA methylation, histone modifications, gene expression, and protein levels from human brain samples. By understanding genetic regulation, Dr. Wang and colleagues provide the Alzheimer’s research community with valuable functional genomics data from well-characterized aging populations, processed through comprehensive multi-omics analyses.\nThe project aims to identify genetic factors influencing Alzheimer’s risk and establish causal pathways linking genetic variation to disease, while building a comprehensive QTL resource to support research on neurodegenerative diseases and brain aging. The initiative successfully mapped molecular traits across different brain cell types—including microglia, astrocytes, and neurons—yielding unprecedented insights into cell-type-specific genetic regulation in Alzheimer’s.\nAccelerating Scientific Breakthroughs with AWS Cloud The FunGen-xQTL project demands infrastructure beyond traditional on-premises capabilities. A major challenge is computational capacity: applying Bayesian models across tens of thousands of genetic variants under hundreds of cell, tissue, ancestry, and disease combinations, covering ~30,000 human genes.\nFunctional genetics also requires processing and analyzing large, high-dimensional datasets to explore relationships between genetic variation and disease onset. Additionally, sharing large-scale datasets across institutions without duplication or copying presents both technical and logistical challenges.\nDr. Wang leverages AWS cloud computing, specifically MMCloud from AWS partner MemVerge, to deploy containerized applications on AWS:\nLarge-scale parallel processing: MMCloud allows the lab to submit hundreds of thousands of jobs on AWS, running cost-effectively on Amazon EC2 Spot Instances. Significant reduction in processing time: complex analyses reduced from weeks to just days. Cost efficiency: using Spot Instances saves 50–80% compared to On-Demand. Simplified collaboration: MMCloud facilitates deployment and management of Jupyter and RStudio applications for multiple academic collaborators. Harnessing AWS Cloud for Scientific Breakthroughs Cloud computing enables researchers like Dr. Wang to:\nEfficiently process and analyze large genomics datasets. Accelerate research publication and scientific impact. Apply complex statistical and computational biology models to understand functional genetic variation. Collaborate globally via data and results sharing. Scale resources up or down on demand, ensuring cost-effectiveness while providing flexibility for advanced research in a rapidly evolving field. Conclusion Dr. Gao Wang’s research at Columbia University, powered by AWS Cloud, represents a modern approach to understanding Alzheimer’s through functional genetics. His work offers hope for developing effective therapies and ultimately finding ways to treat this devastating disease.\nAbout the Authors Meghan Buder – Principal Technical Business Development Manager at AWS, helping researchers leverage cloud to drive innovation. Previously worked in education and holds a Master’s in Mind, Brain, and Education from Harvard Graduate School of Education.\nKarthik Narasimhan – Senior Business Development Manager for genomics and life sciences at AWS, supporting researchers at academic institutions to accelerate research. Holds a Ph.D. in Biological Sciences from the National University of Singapore and an MBA from the University of Texas at Austin.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "AWS Marketplace Now \u0026ldquo;Awardable\u0026rdquo; for DoD Projects in the P1 Solutions Marketplace By: AWS Public Sector Blog Team\nDate: April 14, 2025\nCategory: Announcements, AWS Marketplace, Compliance, Defense, Government, Public Sector, Security\nAmazon Web Services (AWS) is pleased to announce that AWS Marketplace has been granted “Awardable” status in the Department of Defense (DoD) Platform One (P1) Solutions Marketplace. This designation allows DoD organizations to more easily access and purchase solutions through AWS Marketplace using established procurement pathways.\nMission-Critical Access The P1 Solutions Marketplace is a digital repository of five-minute pitch videos selected through a competitive process to address major government requirements for hardware, software, and services. All “awardable” solutions have been evaluated through a rigorous scoring matrix and competitive review process.\nAWS Marketplace inclusion provides DoD customers with access to over 4,000 trusted vendors that meet federal security and compliance requirements.\nSimplified Procurement Process With this status, DoD organizations can leverage AWS Marketplace vendors to:\nAccess pre-approved solutions via existing contract vehicles Accelerate technology deployment timelines Maintain compliance with federal procurement regulations Centralize invoice and license management Track and optimize IT spending across departments Josh Weatherly, Global Director of Public Sector Industry Partnerships and Go-to-Market for AWS Marketplace, said:\n“Bringing AWS Marketplace into the P1 Solutions Marketplace is a transformative step in connecting the DoD with commercial innovation…”\nAs DoD leadership drives procurement reform and accelerates adoption of new technologies, AWS Marketplace’s awardable status simplifies and speeds access to advanced innovations for mission-critical objectives.\nMoving Forward with AWS Marketplace DoD organizations can access AWS Marketplace solutions through the P1 Solutions Marketplace portal. A video, “Amazon Web Services, Inc. Streamline Defense Procurement and License Management with AWS Marketplace”, available exclusively for government customers on P1, demonstrates a practical use case.\nAWS Marketplace earned recognition in the competitive P1 Solutions Marketplace selection process, where solutions must demonstrate innovation, scalability, and potential mission impact. This achievement reflects AWS’s ongoing commitment to supporting government and defense technology needs with secure, compliant, and accessible cloud solutions.\nGovernment customers interested in accessing AWS Marketplace via the P1 Solutions Marketplace can create an account on the P1 Marketplace website. For more information on AWS Marketplace solutions for defense and government applications, contact icmp@amazon.com.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders Event Objectives Provide insights on generative AI adoption for enterprises Share best practices for application modernization and cloud migration Demonstrate AI-powered tools that accelerate software development lifecycle (SDLC) Explore security strategies and cloud transformation for VMware environments Enable executives and technical leaders to align AI initiatives with business objectives Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson – Managing Director, ASEAN, AWS Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Hung Nguyen Gia – Head of Solutions Architect, AWS Son Do – Technical Account Manager, AWS Nguyen Van Hai – Director of Software Engineering, Techcombank Phuc Nguyen – Solutions Architect, AWS Alex Tran – AI Director, OCB Nguyen Minh Ngan – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Vinh Nguyen – Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang – Customer Solutions Manager, AWS Taiki Dang – Solutions Architect, AWS Key Highlights Opening \u0026amp; Keynotes Opening remarks by Hon. Government speaker Keynote by Eric Yeo: Strategies for cloud adoption and AI integration in Southeast Asia Customer Keynote 1 – Dr. Jens Lottner: Techcombank’s journey in digital transformation Customer Keynote 2 – Ms. Trang Phung: U2U Network’s innovative approach to fintech solutions AWS Keynote – Jaime Valles: Regional perspective on generative AI and cloud modernization Panel Discussion: Navigating the GenAI Revolution Moderator: Jeff Johnson, AWS Panelists: Vu Van, Nguyen Hoa Binh, Dieter Botha Topics covered: Steering organizations through rapid GenAI advancements Fostering a culture of innovation Aligning AI initiatives with business objectives Managing organizational change for AI adoption Cloud Migration \u0026amp; Modernization Sessions Completing a large-scale migration and modernization with AWS Speakers: Son Do, Nguyen Van Hai Lessons from enterprise migrations Technical best practices and modernization pathways Case study: building robust foundation and strategic roadmap leveraging AWS cloud Modernizing Applications with Generative AI-Powered Tools Speakers: Phuc Nguyen, Alex Tran Amazon Q Developer: Accelerates code generation and improves code quality Seamless integration with IDE, CLI, DevSecOps Automatically generates documentation and unit tests Acts as an intelligent collaborator using LLMs Demo: understanding complex codebases, suggesting optimizations, automating routine tasks Panel Discussion: Application Modernization Moderator: Hung Nguyen Gia, AWS Panelists: Nguyen Minh Ngan, Nguyen Manh Tuyen, Vinh Nguyen Topics: Using AI tools for faster, higher-quality development Aligning application modernization with business transformation goals Automating SDLC tasks and improving software reliability Transforming VMware with AI-driven Cloud Modernization Speaker: Hung Hoang, AWS Accelerating cloud adoption for VMware estates AWS Transform for fast, safe, cost-effective migration Playbook: downtime-aware patterns, roadmap to EKS, RDS, serverless migration AWS Security at Scale Speaker: Taiki Dang, AWS Enhance security across development and production lifecycle Principles: identification, prevention, detection, response, remediation Leveraging generative AI for security analysis and automation Building resilient, scalable, secure cloud environments Key Takeaways Effective adoption of GenAI requires alignment with business strategy AI-powered tools like Amazon Q Developer accelerate SDLC and improve software quality Migration and modernization on AWS provide scalability, resiliency, and operational efficiency VMware migration using AWS Transform enables smooth cloud adoption Security-by-design and AI-assisted operations enhance enterprise cloud security "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “AI/ML/GenAI on AWS Workshop (AWS Cloud Mastery Series #1)” Event Objectives Introduce fundamentals of AI, Machine Learning, and Generative AI on AWS Present AWS AI/ML services and their real-world applications Demonstrate how to build GenAI applications with Amazon Bedrock and AI Agents Explain Retrieval-Augmented Generation (RAG) for enterprise use cases Introduce Bedrock AgentCore for building enterprise-grade AI agents Speakers Lâm Tuấn Kiệt – AI Engineer Đinh Lê Hoàng Anh – AI Engineer Danh Hoàng Hiếu Nghị – DevOp Engineer Key Highlights 1. Machine Learning \u0026amp; AI Fundamentals (Lâm Tuấn Kiệt) Overview of ML and AI concepts Prompt engineering techniques: Few-shot prompting Chain-of-thought prompting Introduction to Retrieval-Augmented Generation (RAG): Enhances accuracy using enterprise data Supports building intelligent AWS AI agents AWS tools available for RAG integration 2. AWS AI Services Overview (Đinh Lê Hoàng Anh) AWS provides a full suite of AI services ready to use without model training:\nAmazon Rekognition\nImage/video analysis, face detection, object detection Amazon Translate\nNeural machine translation for multilingual content Amazon Textract\nExtracts text from documents Maintains original layout and structure Amazon Transcribe\nSpeech-to-text Custom vocabulary and speaker identification Amazon Polly\nConverts text to lifelike speech Amazon Comprehend\nNatural language processing: sentiment, entities, key phrases Amazon Kendra\nEnterprise search with semantic ranking Amazon Lookout Family\nAnomaly detection for industrial and IoT use cases Amazon Personalize\nReal-time personalized recommendations 3. Bedrock AgentCore (Danh Hoàng Hiếu Nghị) Introduction to Bedrock AgentCore, the core technology for building AI agents Automates reasoning, data retrieval, API calls, and task orchestration Integrates tightly with RAG and foundation models in Amazon Bedrock Enables enterprise-grade assistants, customer support bots, and workflow agents Key Takeaways Design Mindset Always begin with the business use case before choosing models Use proper prompt engineering (few-shot, CoT) for higher accuracy High-quality data is essential for effective RAG Technical Architecture AWS AI services reduce operational overhead with fully managed capabilities Bedrock + AgentCore provide a powerful framework for enterprise AI solutions Combine Rekognition, Textract, Kendra, and Transcribe to build complete AI pipelines Modernization Strategy Follow a phased approach: identify use case → choose service → optimize Start with managed AI services to reduce complexity Use RAG for leveraging private enterprise data securely Applying to Work Deploy Amazon Kendra for internal document search Build document processing workflows using Textract + Comprehend Create Bedrock-based AI agents to support customer interactions Apply Transcribe → Comprehend → Kendra workflow to analyze support calls Use Rekognition for image/video validation or identity verification Event Experience Learning from Experts Covered the full spectrum: ML basics, advanced prompts, and GenAI workflows Gained practical insights into AWS AI service integrations Technical Hands-on Understanding Understood pipelines that combine image, text, audio, and search processing Learned how Bedrock AgentCore and RAG power modern AI applications Networking and Discussions Engaged with experts and attendees to discuss practical AI use cases Improved understanding of selecting the right AWS AI services Lessons Learned AWS provides an end-to-end ecosystem for building AI systems GenAI becomes powerful when combined with internal knowledge bases Bedrock AgentCore simplifies building scalable enterprise AI agents "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “DevOps on AWS (AWS Cloud Mastery Series #2)” Event Objectives Understand DevOps culture and core principles Learn CI/CD pipelines and GitHub workflows Explore Infrastructure as Code with CloudFormation and CDK Learn DevOps best practices, cost optimization, and containerization Compare IaC tools and understand when to use each Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (Timex) Hoang Kha – DevOps Specialist Nguyen Bao – Infrastructure \u0026amp; IaC Expert Nguyen Thinh – AWS CDK Specialist Key Highlights 1. DevOps Culture \u0026amp; Foundations (Truong Quang Tinh) Definition of Dev → DevOps → Operations Core DevOps cultural elements: Collaboration \u0026amp; shared responsibility Automate everything Continuous learning \u0026amp; experimentation Measurement Next-generation DevOps includes: DevOps, Cloud, Platform Engineering, SRE DevOps success metrics: deployment health, agility, and measurable performance DevOps journey overview 2. CI/CD \u0026amp; DevOps Practices (Hoang Kha) CI/CD pipeline fundamentals GitHub workflow optimization Cost optimization strategies Using open-source in DevOps DevOps practices and CI explained: Repo organization Developer workflow Continuous Delivery principles Centralized CI, branching/merging best practices Common reasons builds fail Testing at every stage CI with containers: modular container design Code rules: Branching standards Pull request workflow Naming conventions Source code organization 3. Infrastructure as Code (IaC) with CloudFormation (Nguyen Bao) ClickOps vs IaC (use code, not clicks) CloudFormation overview: IaC tool on AWS Stacks \u0026amp; templates YAML template anatomy How CloudFormation works, Git integration Configuration drift \u0026amp; drift detection 4. AWS CDK – Modern IaC (Nguyen Thinh) Constructs (L1, L2, L3) App and Stack structure Using AWS CLI with profiles AWS Amplify runs on CloudFormation + CDK Production vs Sandbox stacks Notes on sequence diagrams, SDK circular dependency 5. Choosing IaC Tools (Nguyen Bao) Terraform OpenTofu Pulumi Alternative solutions ACC Sandbox (mentioned without details) Key Takeaways Design Mindset DevOps requires cultural change, not just tools Automation and shared responsibility increase reliability Continuous experimentation accelerates innovation Technical Architecture CI/CD pipelines ensure quality and fast delivery IaC eliminates configuration drift Containers improve modularity and scalability CDK simplifies CloudFormation with reusable constructs Modernization Strategy Adopt IaC to replace ClickOps Use Git-based workflows for pipelines Apply best practices in branching \u0026amp; testing Choose the right IaC tool for team skillset and workflow Applying to Work Improve team collaboration using DevOps cultural principles Implement CI/CD pipelines using GitHub Actions Replace console clicking with IaC (CloudFormation / CDK) Modularize applications using containers Adopt branching and code organization best practices Evaluate appropriate IaC tools depending on project needs Event Experience Attending the “DevOps on AWS” session provided a comprehensive understanding of DevOps culture, CI/CD best practices, and the role of IaC in modern cloud engineering. The speakers offered practical insights on real DevOps workflows, improved collaboration models, and hands-on tools such as CloudFormation, CDK, and GitHub workflows.\nThe discussion around containerization, testing, branching strategies, and deployment metrics gave me clear direction for improving software delivery efficiency.\nOverall, the event was highly valuable in strengthening both theoretical understanding and real-world DevOps practices.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “AWS Cloud Mastery Series #3” Event Objectives Learn core AWS Identity \u0026amp; Access Management (IAM) concepts. Understand multi-layer security in AWS environments. Explore GuardDuty, Security Hub, KMS, encryption, and incident response. Gain practical guidance on access control, threat detection, and data protection. Improve cloud security posture across organizations. Speakers Chan Doan Cong Ly Le Duc Anh TranHuynh Hoang Long Hoang Kevin Tran Duc Anh Nguyen Tuan Thinh Nguyen Do Thanh Dat Thinh Dat Kha Van Thinh Lam Viet Nguyen Long Mendel Grabski Tinh Truong Key Highlights IAM – Identity \u0026amp; Access Management (TranHuynh Hoang Long) Manage users and groups. Best practices: avoid \u0026ldquo;*\u0026rdquo;, apply least privilege principle, enable MFA. AWS login security. SSO for multi-application authentication. Service Control Policies (Hoang) Organizational-level policies for business control. Permission Boundaries to limit maximum permissions. Credential Spectrum: long-term vs short-term (STS). MFA: TOTP, FIDO2. Credential rotation using Secrets Manager. IAM Access Analyzer. CloudTrail Organization-Level (Kevin, Thinh, Dat) Organization-wide visibility. Multi-layer security monitoring. EventBridge: alerting \u0026amp; automation. GuardDuty (Thinh) Business pain points. Data sources from AWS accounts (CloudTrail, etc.). Advanced Protection Plans. Runtime monitoring with GuardDuty Agent. AWS Security Hub (Dat) CSPM: cloud security posture management. Network Security (Kha Van) Common attack vectors: outbound, inbound, east-west. VPC Security Groups: stateful, explicit allow. Application-layer protection. SG Sharing via VPC and Transit Gateway. NACLs at subnet layer. Route 53 DNS security. SG/NACL/firewall against attack vectors. Data Protection \u0026amp; KMS (Thinh Lam, Viet Nguyen) KMS workflow \u0026amp; policy. AWS Managed Key vs CMK (customer-managed key). Key rotation cycles. Data classification. Access guardrails. Encryption \u0026amp; Secrets (Viet Nguyen) Encryption in Transit: S3, DynamoDB. Secrets Management via ACM. Incident Response (Mendel, Tinh Truong) Modern threats require modern solutions. Incident controls and security foundations. Prevention strategies. Key Takeaways IAM is the foundation of AWS security; least privilege and MFA are mandatory. SCPs and permission boundaries help enforce organizational guardrails. GuardDuty + CloudTrail + EventBridge create an automated threat detection workflow. Security groups and NACLs play different roles and must be combined effectively. KMS and proper key rotation ensure strong data protection practices. Multi-layer security requires identity, network, data, and incident response integration. Modern cloud security is automated, agent-based, and policy-driven. Applying to Work Apply least privilege, MFA, and secure IAM policies in daily DevOps and cloud operations. Use CloudTrail organization-wide to ensure complete audit coverage. Enable GuardDuty \u0026amp; Security Hub for active threat detection and posture management. Implement VPC security best practices: SGs, NACLs, DNS safeguards. Adopt KMS-managed encryption for sensitive data. Use Secrets Manager and ACM to secure credentials and certificates. Implement automation through EventBridge for alerts and remediation. Develop an incident response workflow aligned with AWS best practices. Event Experience The session provided clear, practical insights into AWS enterprise security. Each speaker focused on real-world, actionable practices. The event covered both foundational IAM concepts and advanced security tooling. Valuable for cloud engineers, DevOps, and security analysts seeking practical AWS security knowledge. "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "IoT Architecture Components AWS IoT Core acts as a secure MQTT broker, allowing IoT devices to publish/subscribe messages using certificates and policies. ESP32 devices connect to IoT Core using Wi-Fi and send sensor data (temperature, humidity, etc.) via MQTT topics. Amazon DynamoDB stores IoT messages for further analytics, dashboards, or downstream applications. Workshop Overview In this workshop, you will work with both a physical IoT device and AWS Cloud services.\nESP32 device will act as the IoT sensor node, connecting to AWS IoT Core through MQTT using X.509 certificates. AWS IoT Core will receive sensor data from ESP32. An IoT Rule will be configured to forward the incoming MQTT messages. DynamoDB Table will store the incoming device data. This simulates a backend environment where IoT data can be queried or processed by external systems. By the end of this workshop, you will have:\nA fully connected ESP32 device sending real-time data to AWS IoT Core. An automated data pipeline that stores incoming messages into DynamoDB. A basic foundation to extend into real IoT applications such as monitoring dashboards, alerting, or data analytics. "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Understand basic AWS services. Create AWS Free Tier account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 09/08/2025 09/08/2025 https://www.youtube.com/@AWSStudyGroup 3 - Create AWS Free Tier account - Practice: + Create new AWS account + MFA for AWS + Create Admin Group and Admin User account 09/09/2025 11/09/2025 https://000001.awsstudygroup.com/ Sat - Managing Costs with AWS Budgets - Practice: + Create Budget by Template + Create Cost Budget + Create RI Budget + Create Usage Budget + Create Savings Plans Budget; 09/13/2025 09/13/2025 https://000007.awsstudygroup.com/ Sun Networking Essentials with Amazon Virtual Private Cloud (VPC) - Practice: + Introduction to Amazon VPC + Firewall in VPC 09/14/2025 09/15/2025 https://000003.awsstudygroup.com/1-introduce/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nUnderstood how to manage cost with AWS Budgets.\nLearn about VPC.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand networking essentials with Amazon Virtual Private Cloud (VPC). Learn about compute essentials with Amazon Elastic Compute Cloud (EC2). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue basic step with VPC - Practice: + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs 09/15/2025 09/15/2025 https://000003.awsstudygroup.com/3-prerequisite/ 3 - Deploying Amazon EC2 Instances - Practice: + Create EC2 Server + Test Connection 09/16/2025 09/16/2025 https://000003.awsstudygroup.com/4-createec2server/ 4 - Continue deploying Amazon EC2 Instances - Practice: + Create NAT Gateway + Using Reachability Analyzer + Create EC2 Instance Connect Endpoint 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/4-createec2server/ 6 - Continue deploying Amazon EC2 Instances - Practice: + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 09/19/2025 09/19/2025 https://000003.awsstudygroup.com/4-createec2server/ Sat - Setting Up Site-to-Site VPN Connection in AWS - Practice: + Create VPC for VPN + Create EC2 as a Customer Gateway + Create Virtual Private Gateway + Create Customer Gateway\n+ Create VPN Connection + Customer Gateway Configuration + Modify AWS VPN Tunnel + Create Transit Gateway + Create Transit Gateway Attachment + Configure Route Tables 09/20/2025 09/20/2025 https://000003.awsstudygroup.com/5-vpnsitetosite/ Sun - Compute Essentials with Amazon Elastic Compute Cloud (EC2) - Practice: + Create VPC and Security Group for Linux and Window + Launch Microsoft Windows Server 2022 Instance + Launch Amazon Linux Instance + Create Customer Gateway\n+ Amazon EC2 Basic 09/21/2025 09/21/2025 https://000004.awsstudygroup.com/ Week 2 Achievements: Understood about VPC and learn how to:\nCreate VPC Create Subnet Create Internet gateway Create Route table Create Sercurity group I can create EC2 and test connection EC2.\nCreate EC2 Instance Connect Endpoint help EC2 private connect to internet not need public.\nHow to connect an On-premise data center to Amazon VPC using a hard or soft VPN, depending on the specific requirements.\nLaunch Instance\nLaunch Microsoft Windows Server 2022 Instance Launch Amazon Linux Instance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Continue learn about compute essentials with Amazon Elastic Compute Cloud (EC2). Learn how to grant your application access to AWS services. Understand and practice Amazon S3 and RDS. Learn how to building applications on Lightsail. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Recovering Access to Windows Instances + Recovering Access to Linux Instances + Remote Desktop to EC2-Ubuntu + Amazon EBS Snapshots Archive + Share AMI 09/22/2025 09/22/2025 https://000004.awsstudygroup.com/5-amazonec2basic/ 3 - Deploying an AWS User Management Application on Amazon Linux 2 -Deploying Node.js Applications on Amazon EC2 Windows - Practice: + Prepare LAMP Server + Test LAMP server + Configuring the database server + Installing phpMyAdmin + Install Node.js on Amazon Linux 2 + Deploying application on Linux Instance + Install XAMPP on Windows instance + Install Node.js on Windows Instance + Deploy AWS User Management Application on Windows Server + Cost \u0026amp; Usage Governance with IAM 09/23/2025 09/23/2025 https://000004.awsstudygroup.com/6-awsfcjmanagement-linux/ 4 - Granting authorization for an application to access AWS services with an IAM role. - Practice: + Create EC2 Instance + Create S3 bucket + Generate IAM user and access key + Use access key + Create IAM role + Using IAM role 09/24/2025 09/24/2025 https://000048.awsstudygroup.com/ 5 - Learn about Simple Storage Service (S3) - Learn about Amazon Relational Database Service (RDS) Practice + Create S3 and load data to S3 + Enable static website feature + Configuring public access block + Configuring public objects + Test website + Block all public access +Config Amazon CloudFront + Test Amazon Cloudfront + Bucket Versioning + Move objects + Create RDS Security Group (create VPC and EC2 first) + Create DB Subnet Group + Application Deployment + Backup and Restore 09/25/2025 09/25/2025 https://000057.awsstudygroup.com/ https://000005.awsstudygroup.com/ 6 - Amazon Lightsail Workshop - Practice: + Deploy a Database on Lightsail + Deploying a WordPress Instance + Ubuntu Configuration + Networking Configuration + WordPress Configuration + Deploying a WordPress Instance + Networking Configuration + Deploying WordPress + Deploying Prestashop E-Commerce Instance + Networking Configuration + Networking Configuration 09/26/2025 08/26/2025 https://000045.awsstudygroup.com/ Week 3 Achievements: How to grant access to the application through accesskey / secretaccesskey.\nHow to grant access to the application through the IAM Role on EC2.\nUnderstand S3 and RDS, practice:\nCreate S3 and load data to S3 Configuring public access block Move objects on S3 Create RDS database instance Building 3 applications on Lightsail:\nDeploying a WordPress Instance Deploying Prestashop E-Commerce Instance Deploying an Akaunting Instance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn about Lightsail Container Learn how to deploy an application with Amazon EC2 Auto Scaling. Building a DNS hybrid system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue Amazon Lightsail Workshop + Application Security + Create Snapshot + Create Alarm 09/29/2025 09/29/2025 https://000045.awsstudygroup.com/ 3 - Run app on Amamzon Lightsail Container - Practice + Creating a Container Service on AWS + Deploying a Public Image + Creating a Lightsail Instance + AWS CLI Configuration Guide + Installing Docker on Ubuntu for AWS Lightsail + Build your container image + Push your container image + Implement New Deploy 09/30/2025 09/30/2025 https://000046.awsstudygroup.com/ 4 - Learn scaling Applications with EC2 Auto Scaling - Practice: + Setup Network Infrastructure + Launch EC2 Instance + Launch a Database Instance with RDS + Setup data for Database + Deploy Web Server + Prepare metric for Predictive scaling + Create Launch Template + Create Target Group + Create Load Balancer + Test + Create Auto Scaling Group + Test manual scaling solution + Test scheduled scaling solution + Test dynamic scaling solution + Read metrics of predictive scaling solution 10/01/2025 10/01/2025 https://000006.awsstudygroup.com/ 5 - AWS CloudWatch Workshop: + Viewing Metrics + Search expressions + Math expressions + Dynamic Labels + CloudWatch Logs + CloudWatch Logs Insights + CloudWatch Metric Filter + CloudWatch Alarms + CloudWatch Dashboards 10/02/2025 10/02/2025 https://000008.awsstudygroup.com/ Sat - Set up Hybrid DNS with Route 53 Resolver + Generate Key Pair + Initialize CloudFormation Template + Configuring Security Group + Connecting to RDGW + Microsoft AD Deployment + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results 10/04/2025 10/04/2025 https://000010.awsstudygroup.com/ Week 4 Achievements: Successfully deployed and ran applications on Amazon Lightsail Containers, including:\nCreating and configuring a container service on AWS. Deploying public container images. Practicing AWS CLI configuration for managing Lightsail containers. Successfully deploy an application with Amazon EC2 Auto Scaling .\nWorked with AWS CloudWatch for monitoring and alerting..\nConfigured a Hybrid DNS setup using Route 53 Resolver, including:\nGenerating key pairs and deploying CloudFormation templates. Setting up security groups and connecting to RDGW. Deploying Microsoft Active Directory and configuring Route 53 outbound/inbound endpoints. Creating and testing Route 53 Resolver Rules to ensure hybrid DNS resolution between on-premises and AWS environments. "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn how to use the AWS Command Line Interface (CLI) Learn the basics and practice hands-on lab of Amazon ElastiCache - Redis service Learn and pratice how to deploy AWS Managed Directory Service Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn how to use CLI - Practice + Install AWS CLI + View resource via CLI + AWS CLI with Amazon S3 + AWS CLI with Amazon SNS + AWS CLI with IAM + AWS CLI with VPC + AWS CLI with Internet Gateway + Creating EC2 Using AWS CLI 10/06/2025 10/06/2025 https://000011.awsstudygroup.com/ 3 - Learn the basics and practice hands-on lab of Amazon ElastiCache - Redis service + Create subnet group use console and use CLI + Create cluster use console and use CLI + Grant access to the cluster + Connect to cluster node 10/07/2025 10/07/2025 https://000061.awsstudygroup.com/ 4 -Learn and practice the steps to host static web content in an Amazon S3 bucket + Create S3 bucket + Upload example index.html file + Configure Amazon CloudFront - Practice AWS Workspace + Prepare To Deploy Amazon WorkSpaces + Deploy Amazon WorkSpaces + Access WorkSpaces - Browser + Access WorkSpaces - WorkSpaces Client 10/08/2025 10/08/2025 https://000094.awsstudygroup.com/ https://000093.awsstudygroup.com/ 5 - Practice Edge Computing with CloudFront and Lambda@Edge + Create CloudFront Distribution + Add EC2 Origin + Test the application + Test Distribution Invalidations + Error Page Custom Configuration + Create Origin Group + Response Headers + Creating Cache Behavior + Create Lambda@Edge function + Deploy Lambda@Edge function to CloudFront 10/09/2025 10/09/2025 https://000130.awsstudygroup.com/ Sat - Learn and pratice how to deploy AWS Managed Directory Service + Deploy AWS Managed Directory Service + Deploy EC2 + Edit Computer Name + EC2 Configuration + Testing Servers Communication 10/11/2025 10/11/2025 https://000095.awsstudygroup.com/ Week 5 Achievements: AWS CLI and CloudShell\nInstalled and configured AWS CLI Practiced using AWS CloudShell for secure command-line operations Static Website Hosting \u0026amp; WorkSpaces\nHosted static website content using Amazon S3 Configured Amazon CloudFront for global content delivery Deployed and accessed Amazon WorkSpaces via both browser and WorkSpaces Client Edge Computing with CloudFront and Lambda@Edge\nCreated and configured CloudFront Distributions Set up EC2 origins, caching behavior, and custom error pages Developed and deployed Lambda@Edge functions for edge processing Managed Microsoft Active Directory\nDeployed AWS Managed Directory Service Configured and connected EC2 instances "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn and implement WordPress deployment on AWS Cloud Practice creating and managing Virtual Machines Learn and practice VM Import/Export between on-premises and AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Learn and implement Wordpress deployment on AWS Cloud + Installing wordpress on EC2 + Initialize AMI from Webserver Instance + Launch Template + Create Load Balancer + Create Auto Scaling Group + Create DB snapshot + Restore with DB snapshot + Create Cloudfront for Web Server 10/14/2025 10/14/2025 https://000101.awsstudygroup.com/ 4 Practice: Create a New Virtual Machine 10/15/2025 10/15/2025 https://000014.awsstudygroup.com/1-deploy-application-server/ 6 - Practice VM Import/Export + Export Virtual Machine from On-premises + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy Instance from AMI + Setting up S3 bucket ACL + Export virtual machine from AMI 10/17/2025 10/17/2025 https://000014.awsstudygroup.com/ Week 6 Achievements: Successfully deployed WordPress on AWS EC2 Practiced creating and configuring Virtual Machines Completed VM Import/Export workflow: Exported VM from on-premises environment Uploaded and imported to AWS as an AMI Deployed and tested new instances successfully "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn and practice AWS Lambda functions for automation and cost efficiency Set up and use Grafana to monitor AWS resources Learn to manage AWS resources using tags effectively Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Learn and practice about Lambda functions to enhance cost efficiency within the AWS environment. + Incoming Web-hooks slack + Create Tag for Instance + Create Role for Lambda + Function stop instance + Function start instance + Check Result 10/21/2025 10/21/2025 https://000022.awsstudygroup.com/ 4 - Practice use Grafana to monitor our resources on AWS + Installing Grafana + Monitoring with Grafana 10/22/2025 10/22/2025 https://000029.awsstudygroup.com/ 5 - Practice Amazon CloudWatch + Viewing Metrics + Search expressions + Math expressions + Dynamic Labels + CloudWatch Logs 10/23/2025 10/23/2025 https://000036.awsstudygroup.com/ 6 - Practice Managing Resources with Tags + Create EC2 Instance with tag + Managing Tags in AWS Resources + Filter resources by tag + Create a Resource Group 10/24/2025 10/24/2025 https://000027.awsstudygroup.com/ Week 7 Achievements: Implemented Lambda functions to start and stop EC2 instances automatically Configured Slack integration with Lambda using incoming webhooks Installed and set up Grafana to visualize AWS metrics Practiced CloudWatch features such as viewing metrics, math expressions, and dynamic labels Created and managed resource tags and built resource groups for better organization "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Practice controlling EC2 access using Resource Tags and IAM Policies Get familiar with AWS Systems Manager, including Patch Manager, Run Command, and Session Manager Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Pravtice process of controlling access to EC2 services using Resource Tags + Create IAM Policy + Create IAM Role + Switch Roles + Initiating access to EC2 console in AWS Region - Tokyo + Initiating access to EC2 console in AWS Region - North Virginia + Proceed to create EC2 instance when there are no and qualified Tags + Edit Resource Tag on EC2 Instance + Policy Check 10/27/2025 10/27/2025 https://000028.awsstudygroup.com/ 3 - Practice on AWS Systems Manager + Patch Manager + Run Command 10/28/2025 10/28/2025 https://000031.awsstudygroup.com/ 4 - Review for midterm exams 10/29/2025 10/29/2025 5 - Review for midterm exams 10/30/2025 10/30/2025 6 - Learn the basics and practice of Amazon System Manager - Session Manager + Preparing VPC and EC2 + Connect to Public Instance + Enable DNS hostnames + Create VPC Endpoint + Connect to instance + Update IAM Role + Create S3 Bucket + Monitor session logs + Port Forwarding 10/31/2025 10/31/2025 https://000058.awsstudygroup.com/ Week 8 Achievements: Created and tested IAM Policies, Roles, and Tags to restrict EC2 access by region Used AWS Systems Manager to remotely manage and patch EC2 instances Configured and connected to EC2 instances via Session Manager, stored session logs in S3, and performed port forwarding "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand and practice VPC Flow Logs for monitoring network traffic Learn how to delegate billing console access using IAM policies Practice resource usage and cost management with IAM restrictions Practice EBS backup anomaly detection Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Understand and practice the features of VPC Flow Logs + Create stack\n+ Create VPC Flow Logs\n+ Enable VPC Flow Logs\n+ Network Infrastructure Monitoring 11/03/2025 11/03/2025 https://000074.awsstudygroup.com/ 3 - Learn and practice resource Usage and Cost Management with IAM on AWS + Limit by Region + Limit by EC2 family + Limit by instance size + Limited by EBS volume 11/04/2025 11/04/2025 https://000064.awsstudygroup.com/ 4 - Practice automating snapshot archival and management using Data Lifecycle Manager + Using a Single Policy Schedule + Use multiple policy schedules + Review results 11/05/2025 11/05/2025 https://000088.awsstudygroup.com/ 5 - Practice AWS Backup anomaly detection for Amazon EBS volumes + Prepare S3, EBS, CloudFormation + Check created resource + Create backup 11/06/2025 11/06/2025 https://000089.awsstudygroup.com/ 6 - Practice AWS Toolkit for VS Code: Amazon Q \u0026amp; CodeWhisperer + AWS Toolkit for Visual Studio Code + Connecting AWS Accounts + Change AWS Region\n+ Authentication\n+ Interacting with Services 11/07/2025 11/07/2025 https://000087.awsstudygroup.com/ Week 9 Achievements: Successfully created and enabled VPC Flow Logs for network monitoring Configured IAM restrictions to control resource usage by region, instance type, and volume Automated snapshot creation and archival using Data Lifecycle Manager Implemented backup anomaly detection for Amazon EBS Connected AWS account to VS Code Toolkit and explored Amazon Q \u0026amp; CodeWhisperer for development assistance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Introduction to AWS and Basic Cloud Services\n→ Learn about AWS basics, cloud computing models, and explore core services such as EC2, S3, and VPC.\nWeek 2: Working with IAM, EC2, and S3 Essentials\n→ Practice IAM configuration, manage EC2 instances, and store/retrieve objects with Amazon S3.\nWeek 3: Configuring Security Groups, Key Pairs, and EBS Volumes\n→ Understand AWS networking security, create and manage key pairs, and attach EBS volumes to EC2 instances.\nWeek 4: Deploying Web Applications and Elastic IP on AWS\n→ Deploy and manage a web server on EC2, configure Elastic IP, and host static or dynamic websites.\nWeek 5: Database Management and Backup with Amazon RDS\n→ Create and configure RDS databases, perform snapshot backups, and connect RDS with web applications.\nWeek 6: Deploying WordPress and Practicing VM Import/Export\n→ Install and deploy WordPress on AWS EC2, set up AMIs, Load Balancers, and Auto Scaling Groups.\n→ Learn and perform VM Import/Export between on-premises and AWS.\nWeek 7: AWS Lambda Automation, Grafana Monitoring, and Resource Tagging\n→ Automate EC2 start/stop using AWS Lambda and Slack webhooks.\n→ Monitor AWS resources using Grafana and CloudWatch.\n→ Manage AWS resources efficiently with tagging and resource groups.\nWeek 8: Managing EC2 Access and Exploring AWS Systems Manager\nWeek 9: VPC Flow Logs, Billing Access Delegation, Resource Usage Management, and Backup Anomaly Detection\nWeek 10: AWS Security, IAM Role Restrictions, Security Hub, and KMS Encryption\nWeek 11: Data Protection with Amazon S3 and Macie, AWS Cognito Cross-Site, Backup, VPC Peering, and S3 Security Best Practices\nWeek 12: Cost Optimization and AWS Database Practice\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "1. Project Overview Project Title IoT Security Monitoring \u0026amp; Alert System for Smart Home on AWS Cloud Platform Project Description Development of a comprehensive IoT security monitoring and alert system for smart homes, combining hardware sensor integration, advanced security protocols, serverless backend architecture, and real-time dashboard visualization. The system provides end-to-end monitoring capabilities for environmental parameters (temperature, humidity, gas detection, motion) with robust security measures and cloud-based analytics.\nTeam Information Team Size: 5 members (3rd-year university students) Team Composition: 2 IC Design Engineers (Hardware/Firmware specialists) 2 Software Engineers (Backend/Frontend specialists) 1 Security Specialist (Cybersecurity \u0026amp; PKI expert) Project Duration 3 months (September 2025 - November 2025)\nTotal Budget $100 USD allocated across two main components:\nWebApp Development: $10 IoT Hardware \u0026amp; Firmware: $40 Project Scale Smart Home - A typical home with 3 sensor nodes covering critical areas.\n2. Project Objectives Primary Goals Develop IoT device ecosystem for smart home monitoring Implement real-time monitoring system for environmental and security parameters Create scalable serverless backend using AWS managed services Build intuitive dashboard interface for monitoring and device management Key Success Metrics Device Connectivity: 99.9% uptime for IoT device connections Real-time Performance: \u0026lt;100ms latency for critical alerts User Experience: Responsive dashboard accessible on web and mobile Cost Efficiency: Stay within $100 budget while achieving production-ready prototype 3. Technical Architecture 3.1 Hardware Layer (IoT Devices) Microcontroller: ESP32 with integrated WiFi capability Sensors: Temperature/humidity (DHT11), Gas detection (MQ series), MKE-S04 IR fire sensor Communication: MQTT over TLS 1.3 for secure data transmission 3.2 AWS Cloud Services Integration Service Purpose Estimated Monthly Cost AWS IoT Core Device gateway and messaging $3-8 (for smart home) AWS IoT Rules Message routing and filtering Free AWS Lambda Serverless business logic $10.25 Amazon DynamoDB NoSQL database for sensor data $0.36 Amazon S3 Data storage and backup $1-2 Amazon SES Email notification services $1-2 API Gateway RESTful API endpoints $6.25 Amazon Cognito User authentication and authorization $1-2 AWS Amplify Frontend hosting and CI/CD $1-2 Amazon Route 53 DNS and domain management $1-2 3.3 System Architecture Diagram 4. Budget Analysis 4.1 WebApp Development ($10) Frontend Framework: React.js/Vue.js development tools - free Testing \u0026amp; Deployment: AWS S3/CloudFront hosting - $10 Development Libraries: Chart.js, Material-UI, WebSocket libraries - free Documentation \u0026amp; Training: Technical writing resources - free 4.2 IoT Hardware \u0026amp; Firmware ($40) Development Boards: 3x ESP32 development kits for smart home - $15 Sensors \u0026amp; Components: Temperature, humidity, gas, motion sensors - $10 Power Supplies \u0026amp; Enclosures: Housing and power management - $10 Connectivity Components: WiFi modules and antenna - $5 5. Implementation Timeline Phase 1: Foundation (Month 1) Week 1-2: Architecture \u0026amp; Planning System architecture design and documentation AWS account setup and service configuration Development environment preparation Team training on AWS services and security protocols Week 3-4: Core Development Hardware schematic design and component sourcing Basic firmware development for sensor integration Backend API design and initial Lambda functions Database schema design Phase 2: Integration (Month 2) Week 5-6: Hardware-Software Integration Component assembly Device-to-cloud communication implementation Real-time data pipeline development User authentication setup Week 7-8: Frontend Development Dashboard UI/UX design Frontend dashboard development Mobile responsive implementation Phase 3: Testing \u0026amp; Deployment (Month 3) Week 9-10: System Integration Testing End-to-end system testing and validation Performance optimization and load testing User acceptance testing and feedback incorporation Bug fixes and refinements Week 11-12: Production Preparation Documentation completion and technical manual creation Production deployment and monitoring setup Final system validation Project presentation and demonstration preparation 6. Deliverables 6.1 Hardware Deliverables IoT Sensor Boards: 3 fully functional prototypes with integrated sensors for smart home Hardware Documentation: Schematics and assembly instructions Manufacturing Guide: Production-ready documentation for scale-up 6.2 Software Deliverables Backend API: RESTful services with comprehensive documentation Frontend Dashboard: Responsive web application with real-time monitoring Mobile Interface: Progressive Web App (PWA) for mobile device access Database Schema: Optimized data models for time-series sensor data 7. Risk Assessment \u0026amp; Mitigation 7.1 Technical Risks Risk Impact Probability Mitigation Strategy Hardware component delays High Medium Order components early, maintain backup suppliers AWS service cost overrun Medium Low Implement cost monitoring, use free tier effectively Security vulnerability discovery High Low Regular security testing, follow AWS best practices Integration complexity Medium Medium Incremental integration, comprehensive testing 7.2 Project Management Risks Risk Impact Probability Mitigation Strategy Team member unavailability Medium Low Cross-training, documentation, backup assignments Timeline delays Medium Medium Buffer time allocation, milestone tracking Budget constraints High Low Weekly budget tracking, cost optimization 8. Expected Outcomes \u0026amp; Impact 8.1 Technical Achievements Functional IoT System: Production-ready prototype demonstrating all key features Real-time Performance: Sub-second response times for critical alerts and notifications Scalable Architecture: Cloud-native design ready for expansion 8.2 Learning Outcomes Cloud Architecture: Hands-on experience with AWS services and serverless computing IoT Development: Understanding of IoT device integration and firmware development Full-Stack Development: Complete software development lifecycle experience Project Management: Practical experience in agile development and team collaboration 8.3 Future Applications Smart Home Management: Environmental monitoring for smart homes Commercial IoT: Equipment monitoring and predictive maintenance systems Healthcare Monitoring: Patient environment and medical device tracking Environmental Monitoring: Air quality and climate monitoring networks 9. Conclusion This IoT Security Monitoring \u0026amp; Alert System project represents a comprehensive approach to modern IoT development, combining cutting-edge hardware design, robust security protocols, and scalable cloud architecture. With a budget of $100 and a timeline of 3 months, our team of 5 dedicated students will deliver a production-ready prototype that demonstrates best practices in IoT security, cloud computing, and full-stack development.\nThe project not only serves as an excellent learning experience but also creates a foundation for real-world applications in smart homes, industrial monitoring, and environmental protection. By leveraging AWS managed services and focusing on security-first design principles, we aim to create a system that meets both current needs and future scalability requirements.\nProject Contact Information:\nProject Manager: Tran Quang Huy Security Lead: Tran Quang Huy Email: huytqse182122@fpt.edu.vn Project Repository: https://github.com/saltless-bruh/D2F_FCJ This proposal is submitted for consideration and approval. We look forward to the opportunity to demonstrate our technical capabilities and deliver exceptional results within the proposed timeline and budget.\nProject Documents Related Documents\rD2F_FCJ_Proposal.docx\r(229 ko)\r"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.2-prerequiste/",
	"title": "Connect ESP32 With IoT Core",
	"tags": [],
	"description": "",
	"content": "Connect ESP32 using IoT Core In this lab, we will use Singapore ap-southeast-1.\nTo prepare the workshop environment, create IoT Core Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nChoose IoT Core template Choose Create Things Choose Create single thing Choose Next Name the thing Choose Next Choose Recommended Option Choose Next Choose Create Policy Choose Next Name the Policy\nChoose Add Four New Statements Base on the Code to fill the Resource Name Choose the Policy Action like the image\nFill in the Policy Resource with your information\nChoose Create\nChoose Create Policy Choose ESP32_POLICY Choose Create thing IoT Code Add the following code to connect the ESP32.\n{\r#ifndef AWS_CONFIG_H\r#define AWS_CONFIG_H\r#include \u0026lt;pgmspace.h\u0026gt;\r// Wi-Fi (Connect to your hotspot)\r#define WIFI_SSID \u0026#34;ABC\u0026#34;\r#define WIFI_PASSWORD \u0026#34;***\u0026#34;\r// ================== AWS IoT Core ==================\r// ✅ Endpoint trong AWS CLI #define AWS_IOT_ENDPOINT \u0026#34;\u0026#34;\r// ✅ Thing name trùng với AWS Thing\r#define AWS_IOT_CLIENT_ID \u0026#34;ESP32_01\u0026#34;\r// ✅ Topics khớp với policy của bạn\r#define AWS_IOT_PUBLISH_TOPIC \u0026#34;esp32/pub\u0026#34;\r#define AWS_IOT_SUBSCRIBE_TOPIC \u0026#34;esp32/sub\u0026#34;\r// Chứng chỉ\rstatic const char AWS_CERT_CA[] PROGMEM = R\u0026#34;EOF(\r)EOF\u0026#34;;\r//Device Certificate\rstatic const char AWS_CERT_CRT[] PROGMEM = R\u0026#34;KEY(\r)KEY\u0026#34;;\rstatic const char AWS_CERT_PRIVATE[] PROGMEM = R\u0026#34;KEY(\r)KEY\u0026#34;;\r#endif\r} Choose Create Certificate Choose Create Download Four Certificates (Device, Public Key File, Private Key File, CA1) Choose Continue Choose esp32/pub Choose Subcribe "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Understand and practice AWS security topics, role restrictions, Security Hub, and KMS. Enable, navigate, and analyze findings from AWS Security Hub to assess compliance and security posture. Implement encryption at rest using AWS KMS across services such as Amazon S3, AWS CloudTrail, and Amazon Athena. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about IAM Permission Boundary + Create Restriction Policy\n+ Create IAM Limited User\n+ Test IAM User Limits 11/10/2025 11/10/2025 https://000030.awsstudygroup.com/ 3 - Practice create a Role and increase security by setting additional restrictions by IP address and time + Create IAM Group + Create IAM User\n+ Create Admin IAM Role + Configure Switch role + Restrict role access 11/11/2025 11/11/2025 https://000044.awsstudygroup.com/ 4 - Get started with AWS Security Hub + Enable Security Hub + Score for each set of criteria 11/12/2025 11/12/2025 https://000018.awsstudygroup.com/ 5 - Encrypt at rest with AWS KMS + Create Key Management Service + Create Amazon S3 + Create AWS CloudTrail and Amazon Athena + Test and share encrypted data on S3 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Configured IAM Permission Boundaries by creating a Restriction Policy, creating a limited IAM user, and verifying that permission limits were correctly enforced. Created IAM Roles, Groups, and Users; applied enhanced security restrictions such as IP address filtering and time-based access rules; and configured role switching. Enabled AWS Security Hub, reviewed the security score, and analyzed findings from various security and compliance criteria. Created and configured a KMS key, applied encryption at rest for Amazon S3, enabled CloudTrail logging, queried logs with Athena, and successfully tested encrypted data sharing on S3. "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Practice using Amazon S3 and Amazon Macie for data classification and protection Implement AWS Cognito across multiple sites for centralized authentication Become familiar with AWS Backup for creating backup plans, setting notifications, and testing restores Practice establishing VPC Peering connections between two VPCs Learn and implement security best practices for Amazon S3, including encryption, access control, and HTTPS enforcement Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Pratice using S3 bucket and Amazon Macie + Prepare S3 and enable Macie + Create Custom data identifiers + Create a Macie job + Macie job run and findings 11/17/2025 11/17/2025 https://000090.awsstudygroup.com/ 3 - Implementing AWS Cognito Across Sites + Preparing Resources + Explanation about the Code + Deploying and Testing Cognito Cross Sites 11/18/2025 11/18/2025 https://000141.awsstudygroup.com/ 4 - Become familiar with using AWS Backup + PrepareS3 and deploy infrastructure + Create Backup plan + Set up notifications + Test Restore 11/19/2025 11/19/2025 https://000013.awsstudygroup.com/ 5 - Practice establish a VPC Peering connection between two VPCs + Prepare CloudFormation Template, SG, EC2 + Update Network ACL + VPC Peering + Route Tables + Cross-Peer DNS 11/20/2025 11/20/2025 https://000019.awsstudygroup.com/ 6 - Practice security best practices for securing data in Amazon S3 + Prepare CloudFormation Template, Secure Network Access, Generate access key, EC2, S3 + Require HTTPS + Require SSE-S3 Encryption + Block Public ACLs + Configuring S3 Block Public Access + Restrict access to S3 VPC Endpoint 11/21/2025 11/21/2025 https://000069.awsstudygroup.com/ Week 11 Achievements: Successfully configured S3 buckets and Amazon Macie, created custom data identifiers, ran Macie jobs, and reviewed findings Deployed and tested AWS Cognito across sites, ensuring proper authentication and access management Created backup plans using AWS Backup, configured notifications, and successfully tested restore operations Established a VPC Peering connection between two VPCs, updated route tables, network ACLs, and enabled cross-VPC DNS resolution Implemented S3 security best practices: enforced HTTPS, enabled SSE-S3 encryption, blocked public ACLs, configured S3 Block Public Access, and restricted access through VPC endpoints "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Practice connecting multiple VPCs using AWS Transit Gateway, including creating attachments and configuring route tables. Understand and practice AWS cost-saving models: Savings Plans, Reserved Instances, and Reserved DB Instances. Practice granting access permissions to the Billing Console using IAM: create Groups, Policies, and test access controls. Explore and practice basic operations with Amazon DynamoDB: creating tables, reading/writing data, updating, querying, creating Global Secondary Indexes, and using AWS CLI in CloudShell for operations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Using AWS Transit Gateway to connect multiple VPCs + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 11/24/2025 11/24/2025 https://000020.awsstudygroup.com/ 3 - Practice Savings Plan, Reserved Instance and Reserved DB Instance + Savings Plans Recommendation + Purchase Savings Plans + Reserved Instance + Reserved DB Instances 11/25/2025 11/25/2025 https://000042.awsstudygroup.com/ 4 - - Practice delegate access to the billing console\n+ Create IAM User Group\n+ Enable Access\n+ Create IAM Policy\n+ Assign Policy\n+ Access Test 11/26/2025 11/26/2025 https://000075.awsstudygroup.com/ 5 - Learn the basics and practice of Amazon DynamoDB\n+ Create a table + Write data + Read data + Update data + Query data + Create a Global Secondary Index + Query the Global Secondary Index + Use AWS CloudShell + Configure AWS CLI 11/27/2025 11/27/2025 https://000060.awsstudygroup.com/ Week 12 Achievements: Successfully created an AWS Transit Gateway, configured attachments, created a Transit Gateway route table, and updated VPC route tables to connect multiple VPCs. Analyzed and applied Savings Plans Recommendations, purchased Savings Plans, and practiced creating Reserved Instances and Reserved DB Instances for cost optimization. Set up Billing Console access control via IAM: created user groups, enabled access permissions, created custom IAM policies, assigned policies, and successfully tested access. Gained proficiency with Amazon DynamoDB: created tables, inserted data, read and queried data, updated items, created and queried Global Secondary Indexes; also used CloudShell and AWS CLI to perform DynamoDB operations through the command line. "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.3-s3-vpc/",
	"title": "Connect IoT Core with DynamoDB",
	"tags": [],
	"description": "",
	"content": "Using IoT Rule In this section, you will create Table\nChoose Create Rule Name the Rule Choose Next Write the SQL Query SELECT * FROM \u0026rsquo;esp32/pub\u0026rsquo; Choose DynamoDB in Action 1 Create DynamoDB Table Choose Create Table Choose Keys like table details Choose Create New Rule Naming Then Create Choose Next Choose Explore Item "
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Heroku migrates PostgreSQL databases to Amazon Aurora This blog explains how Heroku migrated hundreds of thousands of self-managed PostgreSQL databases on Amazon EC2 to Amazon Aurora PostgreSQL-Compatible Edition. You will learn the challenges of managing large-scale database fleets, how Aurora reduces operational overhead while increasing reliability and scalability, and how Heroku used parallel migration tools and rigorous testing to move databases with minimal customer impact. The article also highlights the benefits of the new architecture, including enhanced observability, AI-enabled database features, and near-zero downtime for customers.\nBlog 2 - Accelerating Alzheimer’s research with large-scale functional genomics on AWS Cloud This blog shows how Dr. Gao Wang’s lab at Columbia University uses AWS Cloud computing to advance Alzheimer’s research through functional genomics. You will learn how large-scale molecular QTL analyses help uncover genetic mechanisms of Alzheimer’s, how cloud-based tools like MMCloud and EC2 Spot Instances enable parallel processing of hundreds of thousands of jobs, and how this approach accelerates discovery, reduces costs, and facilitates global collaboration. The article also discusses integrating multi-omics data to identify therapeutic targets and biomarkers.\nBlog 3 - AWS Marketplace becomes “Awardable” for DoD projects in P1 Solutions Marketplace This blog introduces how AWS Marketplace achieved “Awardable” status in the Department of Defense (DoD) Platform One (P1) Solutions Marketplace. You will learn how this status simplifies DoD access to over 4,000 verified cloud solutions, accelerates technology deployment, ensures compliance with federal procurement regulations, centralizes license management, and enables IT cost optimization. The article also demonstrates how AWS Marketplace supports critical defense missions by connecting DoD teams with innovative commercial technologies.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, 18 September 2025\nLocation: 26th \u0026amp; 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS Workshop (AWS Cloud Mastery Series #1)\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DevOps on AWS AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Connect ESP32 with AWS IoT Core \u0026amp; Store Data to DynamoDB Overview AWS IoT Core enables IoT devices such as the ESP32 to securely connect to the AWS Cloud using MQTT. By integrating IoT Core with Amazon DynamoDB, device data can be stored, queried, and analyzed to build scalable IoT applications.\nIn this workshop, you will learn how to:\nConnect an ESP32 to AWS IoT Core using certificates and MQTT. Publish sensor data from ESP32 to AWS. Use an IoT Rule to automatically store messages in DynamoDB. You will implement two main components:\nESP32 → IoT Core – Register a Thing, create certificates and policy, program the ESP32 to publish MQTT data. IoT Core → DynamoDB – Create a DynamoDB table and configure an IoT Rule to insert data. "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Clean up To avoid unnecessary costs after completing the lab, please delete the resources created during the workshop.\n1. Delete the Device in AWS IoT Core Open AWS IoT Core Go to Manage → Things Select the ESP32 Thing you created Click Delete Also delete the associated Certificates \u0026amp; Policy 2. Delete the IoT Rule Go to Message Routing → Rules Select the Rule you created Click Delete Rule 3. Delete the DynamoDB Table Open DynamoDB Go to Tables Select the table created in the lab Click Delete Table "
},
{
	"uri": "https://phucdat25.github.io/OJT/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd from 08/09/2025 to 08/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learn and pratice about cloud and AWS services, through which I improved my skills in programming, use AWS services, reporting, communication,\u0026hellip;\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://phucdat25.github.io/OJT/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://phucdat25.github.io/OJT/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]