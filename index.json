[
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How Heroku Migrated Hundreds of Thousands of Self-Managed PostgreSQL Databases to Amazon Aurora By: Stefan Pieterse, Rocket (John), Jonathan K. Brown, and Justin Downing\nDate: April 10, 2025\nTopics: Advanced (300), Amazon Aurora, Customer Solutions, Migration, PostgreSQL compatible\nIn this post, we discuss how Heroku migrated multi-tenant PostgreSQL databases from a self-managed PostgreSQL environment running on Amazon Elastic Compute Cloud (Amazon EC2) to Amazon Aurora PostgreSQL-Compatible Edition. Heroku completed this large-scale migration without customer impact, while increasing platform reliability and reducing operational burden. We will examine the previous self-managed architecture, the new design, how hundreds of thousands of databases were migrated, and how customer experience improved as a result.\nOverview of Heroku Heroku is a fully managed platform-as-a-service (PaaS) built on Amazon Web Services (AWS), designed to simplify deploying, operating, and scaling applications. Founded in 2007 and acquired by Salesforce in 2010, Heroku is now the platform of choice for more than 13 million applications—from small startups to large-scale enterprise deployments.\nHeroku not only simplifies deployment and scaling using Dynos (Heroku-managed containers) but also offers data service add-ons such as Heroku Postgres, Apache Kafka on Heroku, and Heroku Key-Value Store. These services handle security, server patching, failovers, backups, and other complex configurations, enabling customers to focus on building applications instead of managing data infrastructure. All Heroku Data Services add-ons can be provisioned with a single CLI command or click, providing deep integration, reliability, and scalability.\nOne such add-on is Heroku Postgres, which provides a scalable, cost-effective PostgreSQL database with automated backups, management, performance optimization, and everything required to run a database. For example, Heroku Connect enables seamless synchronization between Salesforce and Heroku Postgres.\nTo ensure customer applications run smoothly, Heroku invests heavily in platform reliability. In early 2025, the Heroku Data Services team migrated its Heroku Postgres Essential tier from a self-managed environment on Amazon EC2 to Amazon Aurora—eliminating operational overhead and allowing engineers to focus on innovation.\nPrevious Self-Managed PostgreSQL Architecture and Its Challenges The diagram below illustrates Heroku’s previous architecture:\nThe Heroku Data team operates multiple control planes to manage customer resources. When a customer provisions a Heroku Postgres add-on, the control plane sends API requests to AWS to create VPCs, EC2 instances, Amazon Elastic Block Store (Amazon EBS) volumes, and Amazon Simple Storage Service (Amazon S3) paths depending on the add-on plan.\nOnce the infrastructure is ready, the control plane configures PostgreSQL services and manages timelines, credentials, and continuous protection. Heroku uses automation to connect to the instances, set configuration files, and execute commands needed to complete database setup. When the database becomes available, the system injects the connection string into the customer’s application configuration. After that, the control plane continuously monitors instance health, service performance, usage, and telemetry to maintain reliability and performance.\nThis architecture served Heroku for more than 10 years. However, managing a massive fleet of database instances introduced growing complexity and operational overhead. Heroku engineers needed to write extensive code to detect and remediate OS issues, handle PostgreSQL upgrades, and address hardware failures.\nEven with automation, engineers increasingly found themselves spending more time on infrastructure maintenance instead of building new customer-facing features. When evaluating alternatives, Amazon Aurora emerged as the clear solution.\nThe New PostgreSQL Architecture with Aurora Here is the illustration of the new design:\nA key advantage is that Heroku’s control plane is already optimized to interact with AWS Cloud services. The system is inspired by a finite-state machine model—each resource exists in a defined state and transitions to another state based on actions. For example, when provisioning an EC2 instance with specific parameters, it enters a pending state until AWS completes the request, then transitions into running.\nBy choosing Aurora as the backend for Heroku Postgres, the Heroku Data team no longer needs to build and manage server infrastructure, maintain custom AMIs, install and patch operating systems, or migrate to new instance types. When customers request a database, Heroku simply provisions and configures an Aurora cluster; AWS handles everything underneath—including instances, storage, replication, and snapshots.\nThis aligns with Heroku’s philosophy of ephemeralization: achieving more with fewer resources. By reducing the complexity of maintaining custom database infrastructure, the team can prioritize delivering higher-value innovations such as enhanced observability, smarter database support, and improved data interoperability.\nHow Heroku Migrated Over 200,000 Databases The Heroku Data team needed to move more than 200,000 self-managed PostgreSQL databases to Aurora—a significant challenge that they completed in just four months with minimal customer disruption.\nAlthough the team had deep PostgreSQL expertise, Aurora was new to them, so AWS collaborated to train more than 40 Heroku engineers.\nThe team used two control planes in parallel:\nLegacy control plane — for the existing self-managed system Modern control plane — for the new Aurora-based system Heroku developed a dedicated transfer system within the modern control plane using pgcopydb to copy data from the old PostgreSQL system to Aurora. Compared with pg_dump/pg_restore, this approach excels thanks to its parallel operations. The average time for lock, copy, pointer update, and unlock was less than 2.2 minutes (p50).\nTo ensure a smooth migration, the team built comprehensive testing capabilities: full end-to-end migration simulations, data validation, and proactive issue handling before customers were impacted. Migrations were executed continuously, averaging 2,000 databases per day. Thanks to meticulous testing, issues were detected and resolved early.\nWhen the transfer system became production-ready, customers had two options:\nSelf-serve migration — customers initiated the migration by changing their plan Automated migration — the system gradually migrated databases in phases to avoid disruption Heroku successfully migrated hundreds of thousands of databases from self-managed PostgreSQL to Aurora PostgreSQL with minimal impact. They also used AWS Countdown, an AWS Enterprise Support program that prepares teams for large-scale events such as migrations and launches, providing quota management, capacity planning, and operational guidance.\nBenefits and Advantages of the New Architecture Moving to the new database architecture represents a long-term investment in an improved customer experience.\nPreviously, operating a fleet of PostgreSQL databases on EC2 required significant engineering effort—effort that competed with building new features. The new Aurora architecture reduces this operational burden: engineers no longer need to manage OS patches, software updates, PostgreSQL services, or supporting libraries.\nOrganizations now rely on AWS to keep customer databases secure, stable, and always available. Heroku gains more time to invest in new features, respond to customer needs, and strengthen its collaboration with AWS.\nThe new architecture enables Heroku Data to deliver future enhancements such as AI-enabled database administration, auto scaling, sleep mode, near-zero downtime, increased connection limits, expanded storage up to 128 TB, and more. With Heroku’s simplicity, attaching such a powerful database to an application requires only a single line of code.\nMigrating to Amazon Aurora also enhances security: encryption at rest via AWS KMS, encryption in transit via SSL/TLS, automatic patching without maintenance windows, IAM authentication, network isolation, and full auditability via AWS CloudTrail. These enterprise-grade security features are built into Aurora by default without additional configuration.\nConclusion The migration from self-managed PostgreSQL on EC2 to Aurora demonstrates Heroku’s careful planning and close collaboration with AWS. After successfully migrating hundreds of thousands of multi-tenant databases, Heroku’s next step is to migrate single-tenant databases within Heroku Private Spaces to Aurora by the end of 2025.\nAbout the Authors Stefan Pieterse is a Principal Customer Solutions Manager at AWS who has supported many strategic customers through cloud migration and modernization. When not working, he enjoys running and spending time with his three-year-old son.\nJohn “Rocket” Nichols is a Solutions Architect at AWS, where he helps large enterprises build systems that are resilient, high-performance, and cost-optimized. He also produces livestream events and makes wine in his spare time.\nJonathan K. Brown is a Senior Product Manager at Heroku, responsible for Heroku Data Services. With a background spanning aerospace to high-tech startups, he holds BS/MS Engineering and an MBA from UC Berkeley, and enjoys paragliding.\nJustin Downing is a Software Engineer Architect at Salesforce, focused on the architecture and systems behind Heroku Data Services. Outside of work, he explores the outdoors through hiking and skiing.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Accelerating Alzheimer’s Research Through Large-Scale Functional Genomics Analysis Powered by AWS Cloud By: Meghan Buder and Karthik Narasimhan\nDate: April 14, 2025\nCategory: Amazon EC2, Customer Solutions, Healthcare, Public Sector\nAlzheimer’s disease is a progressive neurodegenerative disorder affecting millions worldwide. The Alzheimer’s Association estimates that over 6 million Americans are living with the condition, and this number is projected to reach nearly 13 million by 2050. This growing prevalence has driven global research efforts, with scientists working to understand the disease’s complex mechanisms, identify potential risk factors, and develop more effective therapies.\nAt the forefront of this critical research is Dr. Gao Wang, Associate Professor of Neurology at Columbia University. Dr. Wang leads the Lab of Statistical Functional Genomics, where he and his team leverage the power of Amazon Web Services (AWS) cloud computing to conduct groundbreaking genomics research, decoding the complex genetic underpinnings of Alzheimer’s and identifying potential therapeutic targets.\nFunctional Genetics in Alzheimer’s Research Dr. Wang’s research focuses on functional genetics, aiming not only to identify genetic variants associated with Alzheimer’s but also to understand why these variants contribute to disease onset. This approach requires analyzing high-dimensional, large-scale data using sophisticated computational and statistical biology models.\nWith cloud computing, Dr. Wang accelerates analyses, enabling deeper insights and faster progress in understanding Alzheimer’s complex mechanisms. He emphasizes:\n“If the entire field can produce results faster and if those results are meaningful, you can inspire subsequent research and advance the field, hopefully years ahead. Other researchers rely on the data we produce or the findings we generate first, as we can open questions that allow others to focus on their research.”\nThe FunGen-xQTL Project: A Collaborative Effort A major initiative led by Dr. Wang is the FunGen-xQTL Project, a collaboration across 14 research institutions, 28 trainees, and 19 faculty members in the U.S. The project studies molecular quantitative trait loci (QTLs) in aging brains across 62 different molecular contexts in cells and brain tissues.\nThe team generated comprehensive molecular profiles, including DNA methylation, histone modifications, gene expression, and protein levels from human brain samples. By understanding genetic regulation, Dr. Wang and colleagues provide the Alzheimer’s research community with valuable functional genomics data from well-characterized aging populations, processed through comprehensive multi-omics analyses.\nThe project aims to identify genetic factors influencing Alzheimer’s risk and establish causal pathways linking genetic variation to disease, while building a comprehensive QTL resource to support research on neurodegenerative diseases and brain aging. The initiative successfully mapped molecular traits across different brain cell types—including microglia, astrocytes, and neurons—yielding unprecedented insights into cell-type-specific genetic regulation in Alzheimer’s.\nAccelerating Scientific Breakthroughs with AWS Cloud The FunGen-xQTL project demands infrastructure beyond traditional on-premises capabilities. A major challenge is computational capacity: applying Bayesian models across tens of thousands of genetic variants under hundreds of cell, tissue, ancestry, and disease combinations, covering ~30,000 human genes.\nFunctional genetics also requires processing and analyzing large, high-dimensional datasets to explore relationships between genetic variation and disease onset. Additionally, sharing large-scale datasets across institutions without duplication or copying presents both technical and logistical challenges.\nDr. Wang leverages AWS cloud computing, specifically MMCloud from AWS partner MemVerge, to deploy containerized applications on AWS:\nLarge-scale parallel processing: MMCloud allows the lab to submit hundreds of thousands of jobs on AWS, running cost-effectively on Amazon EC2 Spot Instances. Significant reduction in processing time: complex analyses reduced from weeks to just days. Cost efficiency: using Spot Instances saves 50–80% compared to On-Demand. Simplified collaboration: MMCloud facilitates deployment and management of Jupyter and RStudio applications for multiple academic collaborators. Harnessing AWS Cloud for Scientific Breakthroughs Cloud computing enables researchers like Dr. Wang to:\nEfficiently process and analyze large genomics datasets. Accelerate research publication and scientific impact. Apply complex statistical and computational biology models to understand functional genetic variation. Collaborate globally via data and results sharing. Scale resources up or down on demand, ensuring cost-effectiveness while providing flexibility for advanced research in a rapidly evolving field. Conclusion Dr. Gao Wang’s research at Columbia University, powered by AWS Cloud, represents a modern approach to understanding Alzheimer’s through functional genetics. His work offers hope for developing effective therapies and ultimately finding ways to treat this devastating disease.\nAbout the Authors Meghan Buder – Principal Technical Business Development Manager at AWS, helping researchers leverage cloud to drive innovation. Previously worked in education and holds a Master’s in Mind, Brain, and Education from Harvard Graduate School of Education.\nKarthik Narasimhan – Senior Business Development Manager for genomics and life sciences at AWS, supporting researchers at academic institutions to accelerate research. Holds a Ph.D. in Biological Sciences from the National University of Singapore and an MBA from the University of Texas at Austin.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "AWS Marketplace Now \u0026ldquo;Awardable\u0026rdquo; for DoD Projects in the P1 Solutions Marketplace By: AWS Public Sector Blog Team\nDate: April 14, 2025\nCategory: Announcements, AWS Marketplace, Compliance, Defense, Government, Public Sector, Security\nAmazon Web Services (AWS) is pleased to announce that AWS Marketplace has been granted “Awardable” status in the Department of Defense (DoD) Platform One (P1) Solutions Marketplace. This designation allows DoD organizations to more easily access and purchase solutions through AWS Marketplace using established procurement pathways.\nMission-Critical Access The P1 Solutions Marketplace is a digital repository of five-minute pitch videos selected through a competitive process to address major government requirements for hardware, software, and services. All “awardable” solutions have been evaluated through a rigorous scoring matrix and competitive review process.\nAWS Marketplace inclusion provides DoD customers with access to over 4,000 trusted vendors that meet federal security and compliance requirements.\nSimplified Procurement Process With this status, DoD organizations can leverage AWS Marketplace vendors to:\nAccess pre-approved solutions via existing contract vehicles Accelerate technology deployment timelines Maintain compliance with federal procurement regulations Centralize invoice and license management Track and optimize IT spending across departments Josh Weatherly, Global Director of Public Sector Industry Partnerships and Go-to-Market for AWS Marketplace, said:\n“Bringing AWS Marketplace into the P1 Solutions Marketplace is a transformative step in connecting the DoD with commercial innovation…”\nAs DoD leadership drives procurement reform and accelerates adoption of new technologies, AWS Marketplace’s awardable status simplifies and speeds access to advanced innovations for mission-critical objectives.\nMoving Forward with AWS Marketplace DoD organizations can access AWS Marketplace solutions through the P1 Solutions Marketplace portal. A video, “Amazon Web Services, Inc. Streamline Defense Procurement and License Management with AWS Marketplace”, available exclusively for government customers on P1, demonstrates a practical use case.\nAWS Marketplace earned recognition in the competitive P1 Solutions Marketplace selection process, where solutions must demonstrate innovation, scalability, and potential mission impact. This achievement reflects AWS’s ongoing commitment to supporting government and defense technology needs with secure, compliant, and accessible cloud solutions.\nGovernment customers interested in accessing AWS Marketplace via the P1 Solutions Marketplace can create an account on the P1 Marketplace website. For more information on AWS Marketplace solutions for defense and government applications, contact icmp@amazon.com.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders Event Objectives Provide insights on generative AI adoption for enterprises Share best practices for application modernization and cloud migration Demonstrate AI-powered tools that accelerate software development lifecycle (SDLC) Explore security strategies and cloud transformation for VMware environments Enable executives and technical leaders to align AI initiatives with business objectives Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson – Managing Director, ASEAN, AWS Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Hung Nguyen Gia – Head of Solutions Architect, AWS Son Do – Technical Account Manager, AWS Nguyen Van Hai – Director of Software Engineering, Techcombank Phuc Nguyen – Solutions Architect, AWS Alex Tran – AI Director, OCB Nguyen Minh Ngan – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Vinh Nguyen – Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang – Customer Solutions Manager, AWS Taiki Dang – Solutions Architect, AWS Key Highlights Opening \u0026amp; Keynotes Opening remarks by Hon. Government speaker Keynote by Eric Yeo: Strategies for cloud adoption and AI integration in Southeast Asia Customer Keynote 1 – Dr. Jens Lottner: Techcombank’s journey in digital transformation Customer Keynote 2 – Ms. Trang Phung: U2U Network’s innovative approach to fintech solutions AWS Keynote – Jaime Valles: Regional perspective on generative AI and cloud modernization Panel Discussion: Navigating the GenAI Revolution Moderator: Jeff Johnson, AWS Panelists: Vu Van, Nguyen Hoa Binh, Dieter Botha Topics covered: Steering organizations through rapid GenAI advancements Fostering a culture of innovation Aligning AI initiatives with business objectives Managing organizational change for AI adoption Cloud Migration \u0026amp; Modernization Sessions Completing a large-scale migration and modernization with AWS Speakers: Son Do, Nguyen Van Hai Lessons from enterprise migrations Technical best practices and modernization pathways Case study: building robust foundation and strategic roadmap leveraging AWS cloud Modernizing Applications with Generative AI-Powered Tools Speakers: Phuc Nguyen, Alex Tran Amazon Q Developer: Accelerates code generation and improves code quality Seamless integration with IDE, CLI, DevSecOps Automatically generates documentation and unit tests Acts as an intelligent collaborator using LLMs Demo: understanding complex codebases, suggesting optimizations, automating routine tasks Panel Discussion: Application Modernization Moderator: Hung Nguyen Gia, AWS Panelists: Nguyen Minh Ngan, Nguyen Manh Tuyen, Vinh Nguyen Topics: Using AI tools for faster, higher-quality development Aligning application modernization with business transformation goals Automating SDLC tasks and improving software reliability Transforming VMware with AI-driven Cloud Modernization Speaker: Hung Hoang, AWS Accelerating cloud adoption for VMware estates AWS Transform for fast, safe, cost-effective migration Playbook: downtime-aware patterns, roadmap to EKS, RDS, serverless migration AWS Security at Scale Speaker: Taiki Dang, AWS Enhance security across development and production lifecycle Principles: identification, prevention, detection, response, remediation Leveraging generative AI for security analysis and automation Building resilient, scalable, secure cloud environments Key Takeaways Effective adoption of GenAI requires alignment with business strategy AI-powered tools like Amazon Q Developer accelerate SDLC and improve software quality Migration and modernization on AWS provide scalability, resiliency, and operational efficiency VMware migration using AWS Transform enables smooth cloud adoption Security-by-design and AI-assisted operations enhance enterprise cloud security "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “AI/ML/GenAI on AWS Workshop (AWS Cloud Mastery Series #1)” Event Objectives Introduce fundamentals of AI, Machine Learning, and Generative AI on AWS Present AWS AI/ML services and their real-world applications Demonstrate how to build GenAI applications with Amazon Bedrock and AI Agents Explain Retrieval-Augmented Generation (RAG) for enterprise use cases Introduce Bedrock AgentCore for building enterprise-grade AI agents Speakers Lâm Tuấn Kiệt – AI Engineer Đinh Lê Hoàng Anh – AI Engineer Danh Hoàng Hiếu Nghị – DevOp Engineer Key Highlights 1. Machine Learning \u0026amp; AI Fundamentals (Lâm Tuấn Kiệt) Overview of ML and AI concepts Prompt engineering techniques: Few-shot prompting Chain-of-thought prompting Introduction to Retrieval-Augmented Generation (RAG): Enhances accuracy using enterprise data Supports building intelligent AWS AI agents AWS tools available for RAG integration 2. AWS AI Services Overview (Đinh Lê Hoàng Anh) AWS provides a full suite of AI services ready to use without model training:\nAmazon Rekognition\nImage/video analysis, face detection, object detection Amazon Translate\nNeural machine translation for multilingual content Amazon Textract\nExtracts text from documents Maintains original layout and structure Amazon Transcribe\nSpeech-to-text Custom vocabulary and speaker identification Amazon Polly\nConverts text to lifelike speech Amazon Comprehend\nNatural language processing: sentiment, entities, key phrases Amazon Kendra\nEnterprise search with semantic ranking Amazon Lookout Family\nAnomaly detection for industrial and IoT use cases Amazon Personalize\nReal-time personalized recommendations 3. Bedrock AgentCore (Danh Hoàng Hiếu Nghị) Introduction to Bedrock AgentCore, the core technology for building AI agents Automates reasoning, data retrieval, API calls, and task orchestration Integrates tightly with RAG and foundation models in Amazon Bedrock Enables enterprise-grade assistants, customer support bots, and workflow agents Key Takeaways Design Mindset Always begin with the business use case before choosing models Use proper prompt engineering (few-shot, CoT) for higher accuracy High-quality data is essential for effective RAG Technical Architecture AWS AI services reduce operational overhead with fully managed capabilities Bedrock + AgentCore provide a powerful framework for enterprise AI solutions Combine Rekognition, Textract, Kendra, and Transcribe to build complete AI pipelines Modernization Strategy Follow a phased approach: identify use case → choose service → optimize Start with managed AI services to reduce complexity Use RAG for leveraging private enterprise data securely Applying to Work Deploy Amazon Kendra for internal document search Build document processing workflows using Textract + Comprehend Create Bedrock-based AI agents to support customer interactions Apply Transcribe → Comprehend → Kendra workflow to analyze support calls Use Rekognition for image/video validation or identity verification Event Experience Learning from Experts Covered the full spectrum: ML basics, advanced prompts, and GenAI workflows Gained practical insights into AWS AI service integrations Technical Hands-on Understanding Understood pipelines that combine image, text, audio, and search processing Learned how Bedrock AgentCore and RAG power modern AI applications Networking and Discussions Engaged with experts and attendees to discuss practical AI use cases Improved understanding of selecting the right AWS AI services Lessons Learned AWS provides an end-to-end ecosystem for building AI systems GenAI becomes powerful when combined with internal knowledge bases Bedrock AgentCore simplifies building scalable enterprise AI agents "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “DevOps on AWS (AWS Cloud Mastery Series #2)” Event Objectives Understand DevOps culture and core principles Learn CI/CD pipelines and GitHub workflows Explore Infrastructure as Code with CloudFormation and CDK Learn DevOps best practices, cost optimization, and containerization Compare IaC tools and understand when to use each Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (Timex) Hoang Kha – DevOps Specialist Nguyen Bao – Infrastructure \u0026amp; IaC Expert Nguyen Thinh – AWS CDK Specialist Key Highlights 1. DevOps Culture \u0026amp; Foundations (Truong Quang Tinh) Definition of Dev → DevOps → Operations Core DevOps cultural elements: Collaboration \u0026amp; shared responsibility Automate everything Continuous learning \u0026amp; experimentation Measurement Next-generation DevOps includes: DevOps, Cloud, Platform Engineering, SRE DevOps success metrics: deployment health, agility, and measurable performance DevOps journey overview 2. CI/CD \u0026amp; DevOps Practices (Hoang Kha) CI/CD pipeline fundamentals GitHub workflow optimization Cost optimization strategies Using open-source in DevOps DevOps practices and CI explained: Repo organization Developer workflow Continuous Delivery principles Centralized CI, branching/merging best practices Common reasons builds fail Testing at every stage CI with containers: modular container design Code rules: Branching standards Pull request workflow Naming conventions Source code organization 3. Infrastructure as Code (IaC) with CloudFormation (Nguyen Bao) ClickOps vs IaC (use code, not clicks) CloudFormation overview: IaC tool on AWS Stacks \u0026amp; templates YAML template anatomy How CloudFormation works, Git integration Configuration drift \u0026amp; drift detection 4. AWS CDK – Modern IaC (Nguyen Thinh) Constructs (L1, L2, L3) App and Stack structure Using AWS CLI with profiles AWS Amplify runs on CloudFormation + CDK Production vs Sandbox stacks Notes on sequence diagrams, SDK circular dependency 5. Choosing IaC Tools (Nguyen Bao) Terraform OpenTofu Pulumi Alternative solutions ACC Sandbox (mentioned without details) Key Takeaways Design Mindset DevOps requires cultural change, not just tools Automation and shared responsibility increase reliability Continuous experimentation accelerates innovation Technical Architecture CI/CD pipelines ensure quality and fast delivery IaC eliminates configuration drift Containers improve modularity and scalability CDK simplifies CloudFormation with reusable constructs Modernization Strategy Adopt IaC to replace ClickOps Use Git-based workflows for pipelines Apply best practices in branching \u0026amp; testing Choose the right IaC tool for team skillset and workflow Applying to Work Improve team collaboration using DevOps cultural principles Implement CI/CD pipelines using GitHub Actions Replace console clicking with IaC (CloudFormation / CDK) Modularize applications using containers Adopt branching and code organization best practices Evaluate appropriate IaC tools depending on project needs Event Experience Attending the “DevOps on AWS” session provided a comprehensive understanding of DevOps culture, CI/CD best practices, and the role of IaC in modern cloud engineering. The speakers offered practical insights on real DevOps workflows, improved collaboration models, and hands-on tools such as CloudFormation, CDK, and GitHub workflows.\nThe discussion around containerization, testing, branching strategies, and deployment metrics gave me clear direction for improving software delivery efficiency.\nOverall, the event was highly valuable in strengthening both theoretical understanding and real-world DevOps practices.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Event Summary Report: “AWS Cloud Mastery Series #3” Event Objectives Learn core AWS Identity \u0026amp; Access Management (IAM) concepts. Understand multi-layer security in AWS environments. Explore GuardDuty, Security Hub, KMS, encryption, and incident response. Gain practical guidance on access control, threat detection, and data protection. Improve cloud security posture across organizations. Speakers Chan Doan Cong Ly Le Duc Anh TranHuynh Hoang Long Hoang Kevin Tran Duc Anh Nguyen Tuan Thinh Nguyen Do Thanh Dat Thinh Dat Kha Van Thinh Lam Viet Nguyen Long Mendel Grabski Tinh Truong Key Highlights IAM – Identity \u0026amp; Access Management (TranHuynh Hoang Long) Manage users and groups. Best practices: avoid \u0026ldquo;*\u0026rdquo;, apply least privilege principle, enable MFA. AWS login security. SSO for multi-application authentication. Service Control Policies (Hoang) Organizational-level policies for business control. Permission Boundaries to limit maximum permissions. Credential Spectrum: long-term vs short-term (STS). MFA: TOTP, FIDO2. Credential rotation using Secrets Manager. IAM Access Analyzer. CloudTrail Organization-Level (Kevin, Thinh, Dat) Organization-wide visibility. Multi-layer security monitoring. EventBridge: alerting \u0026amp; automation. GuardDuty (Thinh) Business pain points. Data sources from AWS accounts (CloudTrail, etc.). Advanced Protection Plans. Runtime monitoring with GuardDuty Agent. AWS Security Hub (Dat) CSPM: cloud security posture management. Network Security (Kha Van) Common attack vectors: outbound, inbound, east-west. VPC Security Groups: stateful, explicit allow. Application-layer protection. SG Sharing via VPC and Transit Gateway. NACLs at subnet layer. Route 53 DNS security. SG/NACL/firewall against attack vectors. Data Protection \u0026amp; KMS (Thinh Lam, Viet Nguyen) KMS workflow \u0026amp; policy. AWS Managed Key vs CMK (customer-managed key). Key rotation cycles. Data classification. Access guardrails. Encryption \u0026amp; Secrets (Viet Nguyen) Encryption in Transit: S3, DynamoDB. Secrets Management via ACM. Incident Response (Mendel, Tinh Truong) Modern threats require modern solutions. Incident controls and security foundations. Prevention strategies. Key Takeaways IAM is the foundation of AWS security; least privilege and MFA are mandatory. SCPs and permission boundaries help enforce organizational guardrails. GuardDuty + CloudTrail + EventBridge create an automated threat detection workflow. Security groups and NACLs play different roles and must be combined effectively. KMS and proper key rotation ensure strong data protection practices. Multi-layer security requires identity, network, data, and incident response integration. Modern cloud security is automated, agent-based, and policy-driven. Applying to Work Apply least privilege, MFA, and secure IAM policies in daily DevOps and cloud operations. Use CloudTrail organization-wide to ensure complete audit coverage. Enable GuardDuty \u0026amp; Security Hub for active threat detection and posture management. Implement VPC security best practices: SGs, NACLs, DNS safeguards. Adopt KMS-managed encryption for sensitive data. Use Secrets Manager and ACM to secure credentials and certificates. Implement automation through EventBridge for alerts and remediation. Develop an incident response workflow aligned with AWS best practices. Event Experience The session provided clear, practical insights into AWS enterprise security. Each speaker focused on real-world, actionable practices. The event covered both foundational IAM concepts and advanced security tooling. Valuable for cloud engineers, DevOps, and security analysts seeking practical AWS security knowledge. "
},
{
	"uri": "https://phucdat25.github.io/OJT/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Phuc Dat\nPhone Number: 0584825767\nEmail: datnpse182125@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Understand basic AWS services. Create AWS Free Tier account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 09/08/2025 09/08/2025 https://www.youtube.com/@AWSStudyGroup 3 - Create AWS Free Tier account - Practice: + Create new AWS account + MFA for AWS + Create Admin Group and Admin User account 09/09/2025 11/09/2025 https://000001.awsstudygroup.com/ Sat - Managing Costs with AWS Budgets - Practice: + Create Budget by Template + Create Cost Budget + Create RI Budget + Create Usage Budget + Create Savings Plans Budget; 09/13/2025 09/13/2025 https://000007.awsstudygroup.com/ Sun Networking Essentials with Amazon Virtual Private Cloud (VPC) - Practice: + Introduction to Amazon VPC + Firewall in VPC 09/14/2025 09/15/2025 https://000003.awsstudygroup.com/1-introduce/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nUnderstood how to manage cost with AWS Budgets.\nLearn about VPC.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand networking essentials with Amazon Virtual Private Cloud (VPC). Learn about compute essentials with Amazon Elastic Compute Cloud (EC2). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue basic step with VPC - Practice: + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs 09/15/2025 09/15/2025 https://000003.awsstudygroup.com/3-prerequisite/ 3 - Deploying Amazon EC2 Instances - Practice: + Create EC2 Server + Test Connection 09/16/2025 09/16/2025 https://000003.awsstudygroup.com/4-createec2server/ 4 - Continue deploying Amazon EC2 Instances - Practice: + Create NAT Gateway + Using Reachability Analyzer + Create EC2 Instance Connect Endpoint 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/4-createec2server/ 6 - Continue deploying Amazon EC2 Instances - Practice: + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 09/19/2025 09/19/2025 https://000003.awsstudygroup.com/4-createec2server/ Sat - Setting Up Site-to-Site VPN Connection in AWS - Practice: + Create VPC for VPN + Create EC2 as a Customer Gateway + Create Virtual Private Gateway + Create Customer Gateway\n+ Create VPN Connection + Customer Gateway Configuration + Modify AWS VPN Tunnel + Create Transit Gateway + Create Transit Gateway Attachment + Configure Route Tables 09/20/2025 09/20/2025 https://000003.awsstudygroup.com/5-vpnsitetosite/ Sun - Compute Essentials with Amazon Elastic Compute Cloud (EC2) - Practice: + Create VPC and Security Group for Linux and Window + Launch Microsoft Windows Server 2022 Instance + Launch Amazon Linux Instance + Create Customer Gateway\n+ Amazon EC2 Basic 09/21/2025 09/21/2025 https://000004.awsstudygroup.com/ Week 2 Achievements: Understood about VPC and learn how to:\nCreate VPC Create Subnet Create Internet gateway Create Route table Create Sercurity group I can create EC2 and test connection EC2.\nCreate EC2 Instance Connect Endpoint help EC2 private connect to internet not need public.\nHow to connect an On-premise data center to Amazon VPC using a hard or soft VPN, depending on the specific requirements.\nLaunch Instance\nLaunch Microsoft Windows Server 2022 Instance Launch Amazon Linux Instance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Continue learn about compute essentials with Amazon Elastic Compute Cloud (EC2). Learn how to grant your application access to AWS services. Understand and practice Amazon S3 and RDS. Learn how to building applications on Lightsail. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Recovering Access to Windows Instances + Recovering Access to Linux Instances + Remote Desktop to EC2-Ubuntu + Amazon EBS Snapshots Archive + Share AMI 09/22/2025 09/22/2025 https://000004.awsstudygroup.com/5-amazonec2basic/ 3 - Deploying an AWS User Management Application on Amazon Linux 2 -Deploying Node.js Applications on Amazon EC2 Windows - Practice: + Prepare LAMP Server + Test LAMP server + Configuring the database server + Installing phpMyAdmin + Install Node.js on Amazon Linux 2 + Deploying application on Linux Instance + Install XAMPP on Windows instance + Install Node.js on Windows Instance + Deploy AWS User Management Application on Windows Server + Cost \u0026amp; Usage Governance with IAM 09/23/2025 09/23/2025 https://000004.awsstudygroup.com/6-awsfcjmanagement-linux/ 4 - Granting authorization for an application to access AWS services with an IAM role. - Practice: + Create EC2 Instance + Create S3 bucket + Generate IAM user and access key + Use access key + Create IAM role + Using IAM role 09/24/2025 09/24/2025 https://000048.awsstudygroup.com/ 5 - Learn about Simple Storage Service (S3) - Learn about Amazon Relational Database Service (RDS) Practice + Create S3 and load data to S3 + Enable static website feature + Configuring public access block + Configuring public objects + Test website + Block all public access +Config Amazon CloudFront + Test Amazon Cloudfront + Bucket Versioning + Move objects + Create RDS Security Group (create VPC and EC2 first) + Create DB Subnet Group + Application Deployment + Backup and Restore 09/25/2025 09/25/2025 https://000057.awsstudygroup.com/ https://000005.awsstudygroup.com/ 6 - Amazon Lightsail Workshop - Practice: + Deploy a Database on Lightsail + Deploying a WordPress Instance + Ubuntu Configuration + Networking Configuration + WordPress Configuration + Deploying a WordPress Instance + Networking Configuration + Deploying WordPress + Deploying Prestashop E-Commerce Instance + Networking Configuration + Networking Configuration 09/26/2025 08/26/2025 https://000045.awsstudygroup.com/ Week 3 Achievements: How to grant access to the application through accesskey / secretaccesskey.\nHow to grant access to the application through the IAM Role on EC2.\nUnderstand S3 and RDS, practice:\nCreate S3 and load data to S3 Configuring public access block Move objects on S3 Create RDS database instance Building 3 applications on Lightsail:\nDeploying a WordPress Instance Deploying Prestashop E-Commerce Instance Deploying an Akaunting Instance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn about Lightsail Container Learn how to deploy an application with Amazon EC2 Auto Scaling. Building a DNS hybrid system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue Amazon Lightsail Workshop + Application Security + Create Snapshot + Create Alarm 09/29/2025 09/29/2025 https://000045.awsstudygroup.com/ 3 - Run app on Amamzon Lightsail Container - Practice + Creating a Container Service on AWS + Deploying a Public Image + Creating a Lightsail Instance + AWS CLI Configuration Guide + Installing Docker on Ubuntu for AWS Lightsail + Build your container image + Push your container image + Implement New Deploy 09/30/2025 09/30/2025 https://000046.awsstudygroup.com/ 4 - Learn scaling Applications with EC2 Auto Scaling - Practice: + Setup Network Infrastructure + Launch EC2 Instance + Launch a Database Instance with RDS + Setup data for Database + Deploy Web Server + Prepare metric for Predictive scaling + Create Launch Template + Create Target Group + Create Load Balancer + Test + Create Auto Scaling Group + Test manual scaling solution + Test scheduled scaling solution + Test dynamic scaling solution + Read metrics of predictive scaling solution 10/01/2025 10/01/2025 https://000006.awsstudygroup.com/ 5 - AWS CloudWatch Workshop: + Viewing Metrics + Search expressions + Math expressions + Dynamic Labels + CloudWatch Logs + CloudWatch Logs Insights + CloudWatch Metric Filter + CloudWatch Alarms + CloudWatch Dashboards 10/02/2025 10/02/2025 https://000008.awsstudygroup.com/ Sat - Set up Hybrid DNS with Route 53 Resolver + Generate Key Pair + Initialize CloudFormation Template + Configuring Security Group + Connecting to RDGW + Microsoft AD Deployment + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results 10/04/2025 10/04/2025 https://000010.awsstudygroup.com/ Week 4 Achievements: Successfully deployed and ran applications on Amazon Lightsail Containers, including:\nCreating and configuring a container service on AWS. Deploying public container images. Practicing AWS CLI configuration for managing Lightsail containers. Successfully deploy an application with Amazon EC2 Auto Scaling .\nWorked with AWS CloudWatch for monitoring and alerting..\nConfigured a Hybrid DNS setup using Route 53 Resolver, including:\nGenerating key pairs and deploying CloudFormation templates. Setting up security groups and connecting to RDGW. Deploying Microsoft Active Directory and configuring Route 53 outbound/inbound endpoints. Creating and testing Route 53 Resolver Rules to ensure hybrid DNS resolution between on-premises and AWS environments. "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn how to use the AWS Command Line Interface (CLI) Learn the basics and practice hands-on lab of Amazon ElastiCache - Redis service Learn and pratice how to deploy AWS Managed Directory Service Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn how to use CLI - Practice + Install AWS CLI + View resource via CLI + AWS CLI with Amazon S3 + AWS CLI with Amazon SNS + AWS CLI with IAM + AWS CLI with VPC + AWS CLI with Internet Gateway + Creating EC2 Using AWS CLI 10/06/2025 10/06/2025 https://000011.awsstudygroup.com/ 3 - Learn the basics and practice hands-on lab of Amazon ElastiCache - Redis service + Create subnet group use console and use CLI + Create cluster use console and use CLI + Grant access to the cluster + Connect to cluster node 10/07/2025 10/07/2025 https://000061.awsstudygroup.com/ 4 -Learn and practice the steps to host static web content in an Amazon S3 bucket + Create S3 bucket + Upload example index.html file + Configure Amazon CloudFront - Practice AWS Workspace + Prepare To Deploy Amazon WorkSpaces + Deploy Amazon WorkSpaces + Access WorkSpaces - Browser + Access WorkSpaces - WorkSpaces Client 10/08/2025 10/08/2025 https://000094.awsstudygroup.com/ https://000093.awsstudygroup.com/ 5 - Practice Edge Computing with CloudFront and Lambda@Edge + Create CloudFront Distribution + Add EC2 Origin + Test the application + Test Distribution Invalidations + Error Page Custom Configuration + Create Origin Group + Response Headers + Creating Cache Behavior + Create Lambda@Edge function + Deploy Lambda@Edge function to CloudFront 10/09/2025 10/09/2025 https://000130.awsstudygroup.com/ Sat - Learn and pratice how to deploy AWS Managed Directory Service + Deploy AWS Managed Directory Service + Deploy EC2 + Edit Computer Name + EC2 Configuration + Testing Servers Communication 10/11/2025 10/11/2025 https://000095.awsstudygroup.com/ Week 5 Achievements: AWS CLI and CloudShell\nInstalled and configured AWS CLI Practiced using AWS CloudShell for secure command-line operations Static Website Hosting \u0026amp; WorkSpaces\nHosted static website content using Amazon S3 Configured Amazon CloudFront for global content delivery Deployed and accessed Amazon WorkSpaces via both browser and WorkSpaces Client Edge Computing with CloudFront and Lambda@Edge\nCreated and configured CloudFront Distributions Set up EC2 origins, caching behavior, and custom error pages Developed and deployed Lambda@Edge functions for edge processing Managed Microsoft Active Directory\nDeployed AWS Managed Directory Service Configured and connected EC2 instances "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn and implement WordPress deployment on AWS Cloud Practice creating and managing Virtual Machines Learn and practice VM Import/Export between on-premises and AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Learn and implement Wordpress deployment on AWS Cloud + Installing wordpress on EC2 + Initialize AMI from Webserver Instance + Launch Template + Create Load Balancer + Create Auto Scaling Group + Create DB snapshot + Restore with DB snapshot + Create Cloudfront for Web Server 10/14/2025 10/14/2025 https://000101.awsstudygroup.com/ 4 Practice: Create a New Virtual Machine 10/15/2025 10/15/2025 https://000014.awsstudygroup.com/1-deploy-application-server/ 6 - Practice VM Import/Export + Export Virtual Machine from On-premises + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy Instance from AMI + Setting up S3 bucket ACL + Export virtual machine from AMI 10/17/2025 10/17/2025 https://000014.awsstudygroup.com/ Week 6 Achievements: Successfully deployed WordPress on AWS EC2 Practiced creating and configuring Virtual Machines Completed VM Import/Export workflow: Exported VM from on-premises environment Uploaded and imported to AWS as an AMI Deployed and tested new instances successfully "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn and practice AWS Lambda functions for automation and cost efficiency Set up and use Grafana to monitor AWS resources Learn to manage AWS resources using tags effectively Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Learn and practice about Lambda functions to enhance cost efficiency within the AWS environment. + Incoming Web-hooks slack + Create Tag for Instance + Create Role for Lambda + Function stop instance + Function start instance + Check Result 10/21/2025 10/21/2025 https://000022.awsstudygroup.com/ 4 - Practice use Grafana to monitor our resources on AWS + Installing Grafana + Monitoring with Grafana 10/22/2025 10/22/2025 https://000029.awsstudygroup.com/ 5 - Practice Amazon CloudWatch + Viewing Metrics + Search expressions + Math expressions + Dynamic Labels + CloudWatch Logs 10/23/2025 10/23/2025 https://000036.awsstudygroup.com/ 6 - Practice Managing Resources with Tags + Create EC2 Instance with tag + Managing Tags in AWS Resources + Filter resources by tag + Create a Resource Group 10/24/2025 10/24/2025 https://000027.awsstudygroup.com/ Week 7 Achievements: Implemented Lambda functions to start and stop EC2 instances automatically Configured Slack integration with Lambda using incoming webhooks Installed and set up Grafana to visualize AWS metrics Practiced CloudWatch features such as viewing metrics, math expressions, and dynamic labels Created and managed resource tags and built resource groups for better organization "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Practice controlling EC2 access using Resource Tags and IAM Policies Get familiar with AWS Systems Manager, including Patch Manager, Run Command, and Session Manager Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Pravtice process of controlling access to EC2 services using Resource Tags + Create IAM Policy + Create IAM Role + Switch Roles + Initiating access to EC2 console in AWS Region - Tokyo + Initiating access to EC2 console in AWS Region - North Virginia + Proceed to create EC2 instance when there are no and qualified Tags + Edit Resource Tag on EC2 Instance + Policy Check 10/27/2025 10/27/2025 https://000028.awsstudygroup.com/ 3 - Practice on AWS Systems Manager + Patch Manager + Run Command 10/28/2025 10/28/2025 https://000031.awsstudygroup.com/ 4 - Review for midterm exams 10/29/2025 10/29/2025 5 - Review for midterm exams 10/30/2025 10/30/2025 6 - Learn the basics and practice of Amazon System Manager - Session Manager + Preparing VPC and EC2 + Connect to Public Instance + Enable DNS hostnames + Create VPC Endpoint + Connect to instance + Update IAM Role + Create S3 Bucket + Monitor session logs + Port Forwarding 10/31/2025 10/31/2025 https://000058.awsstudygroup.com/ Week 8 Achievements: Created and tested IAM Policies, Roles, and Tags to restrict EC2 access by region Used AWS Systems Manager to remotely manage and patch EC2 instances Configured and connected to EC2 instances via Session Manager, stored session logs in S3, and performed port forwarding "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand and practice VPC Flow Logs for monitoring network traffic Learn how to delegate billing console access using IAM policies Practice resource usage and cost management with IAM restrictions Practice EBS backup anomaly detection Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Understand and practice the features of VPC Flow Logs + Create stack\n+ Create VPC Flow Logs\n+ Enable VPC Flow Logs\n+ Network Infrastructure Monitoring 11/03/2025 11/03/2025 https://000074.awsstudygroup.com/ 3 - Learn and practice resource Usage and Cost Management with IAM on AWS + Limit by Region + Limit by EC2 family + Limit by instance size + Limited by EBS volume 11/04/2025 11/04/2025 https://000064.awsstudygroup.com/ 4 - Practice automating snapshot archival and management using Data Lifecycle Manager + Using a Single Policy Schedule + Use multiple policy schedules + Review results 11/05/2025 11/05/2025 https://000088.awsstudygroup.com/ 5 - Practice AWS Backup anomaly detection for Amazon EBS volumes + Prepare S3, EBS, CloudFormation + Check created resource + Create backup 11/06/2025 11/06/2025 https://000089.awsstudygroup.com/ 6 - Practice AWS Toolkit for VS Code: Amazon Q \u0026amp; CodeWhisperer + AWS Toolkit for Visual Studio Code + Connecting AWS Accounts + Change AWS Region\n+ Authentication\n+ Interacting with Services 11/07/2025 11/07/2025 https://000087.awsstudygroup.com/ Week 9 Achievements: Successfully created and enabled VPC Flow Logs for network monitoring Configured IAM restrictions to control resource usage by region, instance type, and volume Automated snapshot creation and archival using Data Lifecycle Manager Implemented backup anomaly detection for Amazon EBS Connected AWS account to VS Code Toolkit and explored Amazon Q \u0026amp; CodeWhisperer for development assistance "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Introduction to AWS and Basic Cloud Services\n→ Learn about AWS basics, cloud computing models, and explore core services such as EC2, S3, and VPC.\nWeek 2: Working with IAM, EC2, and S3 Essentials\n→ Practice IAM configuration, manage EC2 instances, and store/retrieve objects with Amazon S3.\nWeek 3: Configuring Security Groups, Key Pairs, and EBS Volumes\n→ Understand AWS networking security, create and manage key pairs, and attach EBS volumes to EC2 instances.\nWeek 4: Deploying Web Applications and Elastic IP on AWS\n→ Deploy and manage a web server on EC2, configure Elastic IP, and host static or dynamic websites.\nWeek 5: Database Management and Backup with Amazon RDS\n→ Create and configure RDS databases, perform snapshot backups, and connect RDS with web applications.\nWeek 6: Deploying WordPress and Practicing VM Import/Export\n→ Install and deploy WordPress on AWS EC2, set up AMIs, Load Balancers, and Auto Scaling Groups.\n→ Learn and perform VM Import/Export between on-premises and AWS.\nWeek 7: AWS Lambda Automation, Grafana Monitoring, and Resource Tagging\n→ Automate EC2 start/stop using AWS Lambda and Slack webhooks.\n→ Monitor AWS resources using Grafana and CloudWatch.\n→ Manage AWS resources efficiently with tagging and resource groups.\nWeek 8: Managing EC2 Access and Exploring AWS Systems Manager\nWeek 9: VPC Flow Logs, Billing Access Delegation, Resource Usage Management, and Backup Anomaly Detection\nWeek 10: AWS Security, IAM Role Restrictions, Security Hub, and KMS Encryption\nWeek 11: Data Protection with Amazon S3 and Macie, AWS Cognito Cross-Site, Backup, VPC Peering, and S3 Security Best Practices\nWeek 12: Cost Optimization and AWS Database Practice\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://phucdat25.github.io/OJT/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Understand and practice AWS security topics, role restrictions, Security Hub, and KMS. Enable, navigate, and analyze findings from AWS Security Hub to assess compliance and security posture. Implement encryption at rest using AWS KMS across services such as Amazon S3, AWS CloudTrail, and Amazon Athena. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about IAM Permission Boundary + Create Restriction Policy\n+ Create IAM Limited User\n+ Test IAM User Limits 11/10/2025 11/10/2025 https://000030.awsstudygroup.com/ 3 - Practice create a Role and increase security by setting additional restrictions by IP address and time + Create IAM Group + Create IAM User\n+ Create Admin IAM Role + Configure Switch role + Restrict role access 11/11/2025 11/11/2025 https://000044.awsstudygroup.com/ 4 - Get started with AWS Security Hub + Enable Security Hub + Score for each set of criteria 11/12/2025 11/12/2025 https://000018.awsstudygroup.com/ 5 - Encrypt at rest with AWS KMS + Create Key Management Service + Create Amazon S3 + Create AWS CloudTrail and Amazon Athena + Test and share encrypted data on S3 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Configured IAM Permission Boundaries by creating a Restriction Policy, creating a limited IAM user, and verifying that permission limits were correctly enforced. Created IAM Roles, Groups, and Users; applied enhanced security restrictions such as IP address filtering and time-based access rules; and configured role switching. Enabled AWS Security Hub, reviewed the security score, and analyzed findings from various security and compliance criteria. Created and configured a KMS key, applied encryption at rest for Amazon S3, enabled CloudTrail logging, queried logs with Athena, and successfully tested encrypted data sharing on S3. "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Practice using Amazon S3 and Amazon Macie for data classification and protection Implement AWS Cognito across multiple sites for centralized authentication Become familiar with AWS Backup for creating backup plans, setting notifications, and testing restores Practice establishing VPC Peering connections between two VPCs Learn and implement security best practices for Amazon S3, including encryption, access control, and HTTPS enforcement Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Pratice using S3 bucket and Amazon Macie + Prepare S3 and enable Macie + Create Custom data identifiers + Create a Macie job + Macie job run and findings 11/17/2025 11/17/2025 https://000090.awsstudygroup.com/ 3 - Implementing AWS Cognito Across Sites + Preparing Resources + Explanation about the Code + Deploying and Testing Cognito Cross Sites 11/18/2025 11/18/2025 https://000141.awsstudygroup.com/ 4 - Become familiar with using AWS Backup + PrepareS3 and deploy infrastructure + Create Backup plan + Set up notifications + Test Restore 11/19/2025 11/19/2025 https://000013.awsstudygroup.com/ 5 - Practice establish a VPC Peering connection between two VPCs + Prepare CloudFormation Template, SG, EC2 + Update Network ACL + VPC Peering + Route Tables + Cross-Peer DNS 11/20/2025 11/20/2025 https://000019.awsstudygroup.com/ 6 - Practice security best practices for securing data in Amazon S3 + Prepare CloudFormation Template, Secure Network Access, Generate access key, EC2, S3 + Require HTTPS + Require SSE-S3 Encryption + Block Public ACLs + Configuring S3 Block Public Access + Restrict access to S3 VPC Endpoint 11/21/2025 11/21/2025 https://000069.awsstudygroup.com/ Week 11 Achievements: Successfully configured S3 buckets and Amazon Macie, created custom data identifiers, ran Macie jobs, and reviewed findings Deployed and tested AWS Cognito across sites, ensuring proper authentication and access management Created backup plans using AWS Backup, configured notifications, and successfully tested restore operations Established a VPC Peering connection between two VPCs, updated route tables, network ACLs, and enabled cross-VPC DNS resolution Implemented S3 security best practices: enforced HTTPS, enabled SSE-S3 encryption, blocked public ACLs, configured S3 Block Public Access, and restricted access through VPC endpoints "
},
{
	"uri": "https://phucdat25.github.io/OJT/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Practice connecting multiple VPCs using AWS Transit Gateway, including creating attachments and configuring route tables. Understand and practice AWS cost-saving models: Savings Plans, Reserved Instances, and Reserved DB Instances. Practice granting access permissions to the Billing Console using IAM: create Groups, Policies, and test access controls. Explore and practice basic operations with Amazon DynamoDB: creating tables, reading/writing data, updating, querying, creating Global Secondary Indexes, and using AWS CLI in CloudShell for operations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Using AWS Transit Gateway to connect multiple VPCs + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 11/24/2025 11/24/2025 https://000020.awsstudygroup.com/ 3 - Practice Savings Plan, Reserved Instance and Reserved DB Instance + Savings Plans Recommendation + Purchase Savings Plans + Reserved Instance + Reserved DB Instances 11/25/2025 11/25/2025 https://000042.awsstudygroup.com/ 4 - - Practice delegate access to the billing console\n+ Create IAM User Group\n+ Enable Access\n+ Create IAM Policy\n+ Assign Policy\n+ Access Test 11/26/2025 11/26/2025 https://000075.awsstudygroup.com/ 5 - Learn the basics and practice of Amazon DynamoDB\n+ Create a table + Write data + Read data + Update data + Query data + Create a Global Secondary Index + Query the Global Secondary Index + Use AWS CloudShell + Configure AWS CLI 11/27/2025 11/27/2025 https://000060.awsstudygroup.com/ Week 12 Achievements: Successfully created an AWS Transit Gateway, configured attachments, created a Transit Gateway route table, and updated VPC route tables to connect multiple VPCs. Analyzed and applied Savings Plans Recommendations, purchased Savings Plans, and practiced creating Reserved Instances and Reserved DB Instances for cost optimization. Set up Billing Console access control via IAM: created user groups, enabled access permissions, created custom IAM policies, assigned policies, and successfully tested access. Gained proficiency with Amazon DynamoDB: created tables, inserted data, read and queried data, updated items, created and queried Global Secondary Indexes; also used CloudShell and AWS CLI to perform DynamoDB operations through the command line. "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://phucdat25.github.io/OJT/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Heroku migrates PostgreSQL databases to Amazon Aurora This blog explains how Heroku migrated hundreds of thousands of self-managed PostgreSQL databases on Amazon EC2 to Amazon Aurora PostgreSQL-Compatible Edition. You will learn the challenges of managing large-scale database fleets, how Aurora reduces operational overhead while increasing reliability and scalability, and how Heroku used parallel migration tools and rigorous testing to move databases with minimal customer impact. The article also highlights the benefits of the new architecture, including enhanced observability, AI-enabled database features, and near-zero downtime for customers.\nBlog 2 - Accelerating Alzheimer’s research with large-scale functional genomics on AWS Cloud This blog shows how Dr. Gao Wang’s lab at Columbia University uses AWS Cloud computing to advance Alzheimer’s research through functional genomics. You will learn how large-scale molecular QTL analyses help uncover genetic mechanisms of Alzheimer’s, how cloud-based tools like MMCloud and EC2 Spot Instances enable parallel processing of hundreds of thousands of jobs, and how this approach accelerates discovery, reduces costs, and facilitates global collaboration. The article also discusses integrating multi-omics data to identify therapeutic targets and biomarkers.\nBlog 3 - AWS Marketplace becomes “Awardable” for DoD projects in P1 Solutions Marketplace This blog introduces how AWS Marketplace achieved “Awardable” status in the Department of Defense (DoD) Platform One (P1) Solutions Marketplace. You will learn how this status simplifies DoD access to over 4,000 verified cloud solutions, accelerates technology deployment, ensures compliance with federal procurement regulations, centralizes license management, and enables IT cost optimization. The article also demonstrates how AWS Marketplace supports critical defense missions by connecting DoD teams with innovative commercial technologies.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://phucdat25.github.io/OJT/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, 18 September 2025\nLocation: 26th \u0026amp; 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS Workshop (AWS Cloud Mastery Series #1)\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DevOps on AWS AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://phucdat25.github.io/OJT/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://phucdat25.github.io/OJT/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd from 08/09/2025 to 08/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learn and pratice about cloud and AWS services, through which I improved my skills in programming, use AWS services, reporting, communication,\u0026hellip;\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://phucdat25.github.io/OJT/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\n"
},
{
	"uri": "https://phucdat25.github.io/OJT/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://phucdat25.github.io/OJT/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]